\documentclass[10pt]{article}
\usepackage{titlesec}
\usepackage{geometry}
\geometry{verbose,tmargin=.9in,bmargin=.9in,lmargin=1.0in,rmargin=1.0in}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{url}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[colorlinks=true, linkcolor=red, urlcolor=blue, citecolor=gray]{hyperref}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{etoolbox}
\usepackage{bbm}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\bv}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cost}{Cost}
\DeclareMathOperator{\cut}{cut}
\DeclareMathOperator{\tr}{tr}

\definecolor{nyuDarkPurple}{HTML}{330662}
\definecolor{nyuOfficialPurple}{HTML}{57068c}

\newcommand{\spara}[1]{\vspace{.5em}\noindent {\large\sffamily\textcolor{nyuOfficialPurple}{#1}}}
\titleformat{\section}[hang]{\Large\sffamily\color{nyuDarkPurple}}{\thesection}{1em}{}
\titleformat{\subsection}[hang]{\large\sffamily\color{nyuDarkPurple}}{\thesection}{1em}{}
\titleformat{\subsubsection}[hang]{\normalsize\sffamily\color{gray}}{\thesection}{1em}{}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\includegraphics[width=4cm]{tandon_long_color.eps}}
\rhead{\thepage}
\pagenumbering{gobble}

\setcounter{secnumdepth}{0}

\begin{document}
	
\begin{center}
	\normalsize
	New York University Tandon School of Engineering
	
	Computer Science and Engineering
	\medskip
	
	\large
	CS-GY 6763: Homework 3. 
	
	Due Tuesday, November 21st, 2023, 11:59pm.
	\medskip
	
	\normalsize 
	\noindent \emph{Collaboration is allowed on this problem set, but solutions must be written-up individually. Please list collaborators for each problem separately, or write ``No Collaborators" if you worked alone.}
	\medskip
\end{center} 

\subsection{Problem 1: Concentration of Random Vectors}
\textbf{(10 pts)} In Stochastic Gradient Descent, we replace the true gradient vector with a stochastic gradient that is equal to the true gradient in expectation. Our analysis in class \emph{only used} equality in expectation, although more refined analysis of SGD often requires understanding how well the stochastic gradient \emph{concentrates} around its expectation. Previously, all concentration results we studied apply to random numbers.  For this problem, you will prove a basic concentration inequality for random vectors. 

In particular, let $\bv{x}_1, \ldots, \bv{x}_k \in \R^d$ be i.i.d. random vectors in $d$ dimensions (independent, drawn from the same distribution) with mean $\bs{\mu}$. I.e., $\E[\bv{x}_i] = \bs{\mu}$. Further suppose that $\E\left[\|\bv{x}_i - \bs{\mu}\|_2^2\right] = \sigma^2$. $\sigma^2$ is a natural generalization of ``variance'' to a random vector. Let $\bv{s} = \frac{1}{k}\sum_{i=1}^k\bv{x}_i$. Prove that if $k \geq O(\frac{1/\delta}{\epsilon^2})$ then
\begin{align*}
	\Pr\left[\|\bv{s} - \bs{\mu}\right\|_2 \geq \epsilon \sigma] \leq \delta.
\end{align*}
% \emph{Try first solving the problem first under the assumption that $\bs{\mu} = \bv{0}$.}

\subsection{Problem 2: Regularization}
\textbf{(10 pts)}
Regularization is a popular technique in machine learning. It is often used to improve final test error, but can also help speed-up optimization methods like gradient descent by improving the condition number of the function being regularized. In particular, let $f(\bv{x})$ be a differentiable function mapping a length $d$ vector $\bv{x}$ to a scalar value. Let $g$ be the function with added Euclidean regularization:
	\begin{align*}
		g(\bv{x}) = f(\bv{x}) + \lambda \|\bv{x}\|_2^2
	\end{align*}  Above $\lambda > 0$ is a non-negative constant that controls the amount of regularization. Suppose $f$ is $\alpha_1$-strongly convex and $\beta_1$-smooth, so has condition number $\beta_1/\alpha_1$. Prove that $g$ is also convex and its condition number less than or equal to that of $f$.

\subsection{Problem 3: Separation Oracles}
\textbf{(12 pts)} Describe efficient separation oracles for each of the following families of convex sets.  Here, ``efficient'' means linear time plus $O(1)$ calls to any additional oracles provided to you.

\begin{enumerate}[label=(\alph*)]
	\item
	The set $A \cap B$, given separation oracles for $A$ and $B$.
	
	\item
	The $\ell_1$ ball: $\| \bv{x} \|_1 \leq 1$.
	
	\item
	Any convex set $A$, given a \emph{projection oracle} for $A$. Recall that a projection oracle, given a point $\bv{x}$, returns
	\begin{equation*}
		\text{Proj}_A(\bv{x}) =  \argmin_{y \in A} \| \bv{x} - \bv{y} \|_2.
	\end{equation*}
	
	% \item
	% The $\epsilon$-neighborhood of a convex set $A$:
	% \begin{equation*}
	% 	\{ x \mid \exists y \in A, \| \bv{x} - \bv{y} \|_2 \leq \epsilon \}
	% \end{equation*}
	% given a projection oracle for $A$.
\end{enumerate}
Above you may wish to use the following fact that was stated but not proven in class: for any point $\bv{x}$, convex set $A$, and $\bv{z}\in A$, $\|\bv{z}-\text{Proj}_A(\bv{x})\|_2 \leq \|\bv{z} - \bv{x}\|_2$.




%\subsection{Problem 1: Acceleration Through the Polynomial Lens}
%\textbf{(15 pts)}
%In Lecture 7, we saw how to analyze gradient descent for $f(\bv{x}) = \|\bv{A}\bv{x} - \bv{b}\|_2^2$, which has gradient $\nabla f(\bv{x}) = 2\bv{A}^T\bv{A}\bv{x} - 2\bv{A}^T\bv{b}$. The dominant cost for each gradient descent iteration is multiplying $\bv{x}$ by $\bv{A}^T\bv{A}$ to compute the gradient, which takes $O(nd)$ time when $A$ is $n\times d$. 
%
%We obtained a convergence bound depending on the largest and smallest eigenvalues of $\bv{A}^T\bv{A}$, which we denote $\lambda_1$ and $\lambda_d$ respectively. We did so by rearranging the gradient descent update rule:
%\begin{align*}
%	\bv{x}^{(i)} &= \bv{x}^{(i-1)} - \eta\left(2\bv{A}^T\bv{A}\bv{x}^{(i-1)} - 2\bv{A}^T\bv{b}\right) \\
%	\bv{x}^{(i)} - \bv{x}^* &= \bv{x}^{(i-1)} - \eta\left(2\bv{A}^T\bv{A}\bv{x}^{(i-1)} - 2\bv{A}^T\bv{A}\bv{x}^*\right) -\bv{x}^*&& \text{since $\nabla f(\bv{x}^*)=\bv{0}$, so $\bv{A}^T\bv{A}\bv{x}^* = \bv{A}^T\bv{b}$}\\
%	\bv{x}^{(i)} - \bv{x}^* &= (\bv{I} - 2\eta\bv{A}^T\bv{A})(\bv{x}^{(i-1)} - \bv{x}^*).
%\end{align*}
%By induction, it follows that the error $\bv{x}^{(i)} - \bv{x}^*$ equals $\bv{x}^{(i)} - \bv{x}^* = (\bv{I} - 2\eta\bv{A}^T\bv{A})^i(\bv{x}^{(0)} - \bv{x}^*)$. This allowed us to obtain a convergence bound by arguing that, if we set $\eta = 1/2\lambda_1$ where $\lambda_1$ is the largest eigenvalue of $\bv{A}^T\bv{A}$, then $(\bv{I} - \frac{1}{\lambda_1}\bv{A}^T\bv{A})^i$ has top eigenvalue $< \epsilon$ after $i = O(\frac{\lambda_1}{\lambda_d}\log(1/\epsilon))$ iterations. In this problem you will prove an ``accelerated'' version of this bound that only requires $O(\sqrt{\frac{\lambda_1}{\lambda_d}} \log(1/\epsilon))$ iterations.  
%
%\begin{enumerate}
%	\item Let $p$ be a degree $q$ polynomial. I.e. $p = c_0 + c_1 x + \ldots + c_q x^q$. Show that, for any $p$ with $c_0 + c_1 + \ldots + c_q = 1$ and any starting vector $\bv{x}^{(0)}$, we can compute in $O(ndq)$ time a vector $\bv{x}^{(q)}$ such that:
%	\begin{align*}
%		\bv{x}^{(q)} - \bv{x}^* = p\left(\bv{I} - \frac{1}{\lambda_1}\bv{A}^T\bv{A}\right)(\bv{x}^{0} - \bv{x}^*).
%	\end{align*}
%	\item Prove that for $q = O(\sqrt{\frac{\lambda_1}{\lambda_d}} \log(1/\epsilon))$, there exists a polynomial $p$ with coefficients $c_0 + c_1 + \ldots + c_q = 1$ such that the top eigenvalue of $p\left(\bv{I} - \frac{1}{\lambda_1}\bv{A}^T\bv{A}\right) \leq \epsilon$. \textbf{Hint:} What does this requirement on the coefficients tell us about the value of $p(0)$ and $p(1)$? 
%	\item \textbf{Quick answer:} Write down a convergence bound for your new algorithm based on the above result. 
%\end{enumerate}

\subsection{Problem 4: Gradient Descent with Decaying Step-size}
\textbf{(10 pts)}
In class we showed that gradient descent with step size $\eta = R/G\sqrt{T}$ converges to an $\epsilon$ approximate minimizer of a convex $G$-Lipschitz function in $T = R^2G^2/\epsilon^2$ steps if our starting point $\bv{x}^{(0)}$ satisfies $\|\bv{x}^{(0)} - \bv{x}^*\|_2 \leq R$. 
Choosing this step size requires knowing $G$, $R$ and moreover $T$ in advance, which might not be reasonable in a lot of settings. For example, when training machine learning models, we might not be able to estimate how long it will take to reach a point where test accuracy levels off. Instead, we want to be able to keep running the algorithm, achieving better and better accuracy as we do. 

Here, we analyze a variant of gradient descent with a variable step size that avoids this limitation. In particular, consider running gradient descent with the update $\bv{x}^{(i+1)} = \bv{x}^{(i)}- \eta \nabla f(\bv{x}^{(i)})$, where
\begin{align*}
	\eta = \frac{f(\bv{x}^{(i)}) - f(\bv{x}^{*})}{\|\nabla f(\bv{x}^{(i)})\|_2^2}.
\end{align*}
This step size requires knowledge of $f(\bv{x}^{*})$, but not $\bv{x}^*$, which may be reasonable in some settings. Moreover, since it's just one parameter, grid search can be more easily used to ``guess'' $f(\bv{x}^{*})$ than the three parameters $G,R,T$. More complex approaches can remove the need to know this value entirely.

Prove that, if we run gradient descent for $T = O(R^2G^2/\epsilon^2)$ steps using the step size above then $\hat{\bv{x}} = \min_{i \in 0, \ldots, T}f(\bv{x}^{(i)})$ satisfies $f(\hat{\bv{x}}) \leq f(\bv{x}^*) + \epsilon$. \textbf{Hint:} Prove that our distance from the optimum $\|\bv{x}^{(i)} - \bv{x}^*\|_2$ {always decreases} with this choice of step size, and the decrease is larger if our gap from the objective value $f(\bv{x}^{(i)}) - f(\bv{x}^{*})$ is larger.

\subsection{Problem 5: Locating Points via the SVD}
	
	\textbf{(15 pts)} Suppose you are given all pairs distances between a set of points $\bv{x}_1, \ldots, \bv{x}_n \in \R^d$. You can assume that $d \ll n$. Formally, you are given an $n\times n$ matrix $\bv{D}$ with $\bv{D}_{i,j} = \|\bv{x}_i - \bv{x}_j\|_2^2$. You would like to recover the location of the original points, at least up to possible rotation and translation (which do not change pairwise distances). Since we can only recover up to a translation, it may be easiest to assume that the points are centered around the origin. I.e. that $\sum_{i=1}^n \bv{x}_i = \bv{0}$. 
	\begin{enumerate}[label=(\alph*)]
		\item Under this assumption, describe an efficient algorithm for learning $\sum_{i=1}^n \|\bv{x}_i\|_2^2$ from $\bv{D}$. 
		\item Next, describe an efficient algorithm for learning $\|\bv{x}_i\|_2^2$ for each $i \in 1, \ldots, n$. 
		\item Finally, describe an algorithm for recovering a set of points $\bv{x}_1,\ldots, \bv{x}_n$ which realize the distances in $\bv{D}$. Hint: This is where you will use the SVD! It might help to prove that $\bv{D}$ has rank $\leq d + 2$.
		\item
		Implement your algorithm and run it on the U.S. cities dataset provided in \texttt{UScities.txt}. Note that the distances in the file are unsquared Euclidean distances, so you need to square them to obtain $\bv{D}$. Plot your  estimated city locations on a 2D plot and label the cities to make it clear how the plot is oriented. Submit these images and your code with the problem set.
	\end{enumerate}






\end{document}