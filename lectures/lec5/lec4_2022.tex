 \documentclass[compress]{beamer}

\usetheme[block=fill]{metropolis}

\usepackage{graphicx} % Allows including images
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{color}
\usepackage{xcolor,cancel}
\definecolor{mDarkBrown}{HTML}{604c38}
\definecolor{mDarkTeal}{HTML}{23373b}
\definecolor{mLightBrown}{HTML}{EB811B}
\definecolor{mMediumBrown}{HTML}{C87A2F}
\definecolor{mygreen}{HTML}{98C2B9}
\definecolor{myyellow}{HTML}{DFD79C}
\definecolor{myblue}{HTML}{8CA7CC}
\definecolor{kern}{HTML}{8CC2B7}


\usepackage{float}
\usepackage{framed}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{ulem}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{comment}   
\usepackage{bbm}
\usepackage{tikz}   
\def\Put(#1,#2)#3{\leavevmode\makebox(0,0){\put(#1,#2){#3}}}
\newcommand*\mystrut[1]{\vrule width0pt height0pt depth#1\relax}
\newcommand{\eqdef}{\mathbin{\stackrel{\rm def}{=}}}


\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\bv}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\mv}{mv}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\step}{step}
\DeclareMathOperator{\gap}{gap}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\orth}{orth}
\newcommand{\norm}[1]{\|#1\|}
\captionsetup[subfigure]{labelformat=empty}
\captionsetup[figure]{labelformat=empty}
\DeclareMathOperator*{\lmin}{\lambda_{min}}
\DeclareMathOperator*{\lmax}{\lambda_{max}}

\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newcommand{\specialcellleft}[2][c]{%
\begin{tabular}[#1]{@{}l@{}}#2\end{tabular}
}

\usepackage{tabstackengine}
\stackMath


%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{CS-GY 6763: Lecture 4 \\ Dimensionality reduction + Near neighbor search in high dimensions}
\author{NYU Tandon School of Engineering, Prof. Christopher Musco}
\date{}

\begin{document}

\begin{frame}
	\titlepage 
\end{frame}

\metroset{titleformat=smallcaps}

\begin{frame}
	\frametitle{euclidean dimensionality reduction}
	\begin{lemma}[Johnson-Lindenstrauss, 1984]
		For any set of $n$ data points $\bv{q}_1,\ldots, \bv{q}_n \in \R^d$ there exists a \emph{linear map} $\Pi: \R^d \rightarrow \R^k$ where $k = O\left(\frac{\log n}{\epsilon^2}\right)$ such that \emph{for all $i,j$},
		\begin{align*}
			(1-\epsilon)\|\bv{q}_i - \bv{q}_j\|_2 \leq \|\bs{\Pi}\bv{q}_i - \bs{\Pi}\bv{q}_j\|_2 \leq (1+\epsilon)\|\bv{q}_i - \bv{q}_j\|_2.
		\end{align*}
	\end{lemma}
	\begin{center}
		\includegraphics[height=.45\textheight]{jl_sketch.png}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{euclidean dimensionality reduction}
	\textbf{Please remember:} This is equivalent to: 
	\begin{lemma}[Johnson-Lindenstrauss, 1984]
		For any set of $n$ data points $\bv{q}_1,\ldots, \bv{q}_n \in \R^d$ there exists a \emph{linear map} $\Pi: \R^d \rightarrow \R^k$ where $k = O\left(\frac{\log n}{\epsilon^2}\right)$ such that \emph{for all $i,j$},
		\begin{align*}
			(1-\epsilon)\|\bv{q}_i - \bv{q}_j\|_2^{\mathbf{\alert{2}}} \leq \|\bs{\Pi}\bv{q}_i - \bs{\Pi}\bv{q}_j\|_2^{\mathbf{\alert{2}}} \leq (1+\epsilon)\|\bv{q}_i - \bv{q}_j\|_2^{\mathbf{\alert{2}}}.
		\end{align*}
	\end{lemma}
	because for small $\epsilon$, $(1+\epsilon)^2 = 1 + O(\epsilon)$ and $(1-\epsilon)^2 = 1 - O(\epsilon)$.
\end{frame}

\begin{frame}
	\frametitle{euclidean dimensionality reduction}
	And this is equivalent to:
	\begin{lemma}[Johnson-Lindenstrauss, 1984]
		For any set of $n$ data points $\bv{q}_1,\ldots, \bv{q}_n \in \R^d$ there exists a \emph{linear map} $\Pi: \R^d \rightarrow \R^k$ where $k = O\left(\frac{\log n}{\epsilon^2}\right)$ such that \emph{for all $i,j$},
		\begin{align*}
			(1-\epsilon)\|\bs{\Pi}\bv{q}_i - \bs{\Pi}\bv{q}_j\|_2^{{{2}}} \leq \|\bv{q}_i - \bv{q}_j\|_2^{{{2}}} \leq (1+\epsilon)\|\bs{\Pi}\bv{q}_i - \bs{\Pi}\bv{q}_j\|_2^{{{2}}}.
		\end{align*}
	\end{lemma}
	because for small $\epsilon$, $\frac{1}{1+\epsilon} = 1 - O(\epsilon)$ and $\frac{1}{1-\epsilon} = 1 + O(\epsilon)$.
\end{frame}

\begin{frame}[t]
	\frametitle{sample application}
	\textbf{k-means clustering}: Give data points $\bv{a}_1,\ldots, \bv{a}_n \in \R^d$, find centers $\bs{\mu}_1, \ldots, \bs{\mu}_k\in \R^d$ to minimize:
	\begin{align*}
		Cost(\bs{\mu}_1,\ldots, \bs{\mu}_k) = \sum_{i=1}^n \min_{j = 1,\ldots,k} \|\bs{\mu}_j - \bv{a}_i\|_2^2
	\end{align*}
	\begin{center}
		\includegraphics[width=.5\textwidth]{kmeans1.png}
	\end{center}
\end{frame}
\begin{frame}[t]
	\frametitle{sample application}
	\textbf{k-means clustering}: Give data points $\bv{a}_1,\ldots, \bv{a}_n \in \R^d$, find centers $\bs{\mu}_1, \ldots, \bs{\mu}_k\in \R^d$ to minimize:
	\begin{align*}
		Cost(\bs{\mu}_1,\ldots, \bs{\mu}_k) = \sum_{i=1}^n \min_{j = 1,\ldots,k} \|\bs{\mu}_j - \bv{a}_i\|_2^2
	\end{align*}
	\begin{center}
		\includegraphics[width=.5\textwidth]{kmeans2.png}
	\end{center}
\end{frame}
\begin{frame}[t]
	\frametitle{sample application}
	\textbf{k-means clustering}: Give data points $\bv{a}_1,\ldots, \bv{a}_n \in \R^d$, find centers $\bs{\mu}_1, \ldots, \bs{\mu}_k\in \R^d$ to minimize:
	\begin{align*}
		Cost(\bs{\mu}_1,\ldots, \bs{\mu}_k) = \sum_{i=1}^n \min_{j = 1,\ldots,k} \|\bs{\mu}_j - \bv{a}_i\|_2^2
	\end{align*}
	\begin{center}
		\includegraphics[width=.5\textwidth]{kmeans3.png}
	\end{center}
\end{frame}

\begin{frame}[t]
	\frametitle{k-means clustering}
	NP hard to solve exactly, but there are many good approximation algorithms. All depend at least linearly on the dimension $d$. 
	
	\textbf{Approximation scheme}: Find clusters $\tilde{C}_1, \ldots, \tilde{C}_k$ for the $k = O\left(\frac{\log n}{\epsilon^2}\right)$ dimension data set $\bs{\Pi}\bv{a}_1, \ldots, \bs{\Pi}\bv{a}_n.$
	
	\vspace{-3em}
	\begin{center}
		\includegraphics[width=.6\textwidth]{clustering_projected.png}
	\end{center}
	\vspace{-2em}
	Argue these clusters are near optimal for $\bv{a}_1, \ldots, \bv{a}_n$.
\end{frame}


\begin{frame}[t]
	\frametitle{k-means clustering}
	\textbf{Equivalent form}: Find clusters $C_1, \ldots, C_k \subseteq \{1, \ldots, n\}$ to minimize:
	\vspace{-3em}
	
	\begin{align*}
		Cost(C_1,\ldots, C_k) = \sum_{j=1}^k \frac{1}{2|C_j|}\sum_{u,v\in C_j} \|\bv{a}_u - \bv{a}_v\|_2^2.
	\end{align*}
	\vspace{-2em}
	\begin{center}
		\includegraphics[width=.5\textwidth]{kmeans4.png}
	\end{center}
	\textbf{Exercise:} Prove this to your self.
\end{frame}

\begin{frame}[t]
	\frametitle{k-means clustering}
	\begin{align*}
		Cost(C_1,\ldots, C_k) &= \sum_{j=1}^k \frac{1}{2|C_j|}\sum_{u,v\in C_j} \|\bv{a}_u - \bv{a}_v\|_2^2 \\
		\widetilde{Cost}(C_1,\ldots, C_k) &= \sum_{j=1}^k \frac{1}{2|C_j|}\sum_{u,v\in C_j} \|\Pi\bv{a}_u - \Pi\bv{a}_v\|_2^2
	\end{align*}
	
	
	\textbf{Claim:} For any clusters $C_1, \ldots, C_k$:
	\begin{align*}
		(1-\epsilon) Cost(C_1, \ldots, C_k) \leq \widetilde{Cost}(C_1, \ldots, C_k)   \leq  (1+\epsilon) Cost(C_1, \ldots, C_k) 
	\end{align*}
	
\end{frame}

\begin{frame}[t]
	\frametitle{k-means clustering}
	Suppose we use an approximation algorithm to find clusters $B_1, \ldots, B_k$ such that:
	\begin{align*}
		\widetilde{Cost}(B_1,\ldots, B_k) \leq (1+\alpha)\widetilde{Cost}^*
	\end{align*}
	
	Then: 
	\begin{align*}
		{Cost}(B_1,\ldots, B_k) &\leq \frac{1}{1-\epsilon}\widetilde{Cost}(B_1,\ldots, B_k) \\
		&\leq(1+O(\epsilon)) (1+\alpha)\widetilde{Cost}^*\\
		&\leq (1+O(\epsilon))(1+\alpha)(1+\epsilon){Cost}^*\\
		&= \alert{\left(1 + O(\alpha + \epsilon)\right){Cost}^*}
	\end{align*}
	
	\vspace{1em}
	\begin{block}{\vspace*{-3ex}}
		\small ${Cost}^* = \min_{C_1, \ldots, C_k} Cost(C_1, \ldots, C_k)$ and $\widetilde{Cost}^* = \min_{C_1, \ldots, C_k} \widetilde{Cost}(C_1, \ldots, C_k) $
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{euclidean dimensionality reduction}
	\begin{lemma}[Johnson-Lindenstrauss, 1984]
		For any set of $n$ data points $\bv{q}_1,\ldots, \bv{q}_n \in \R^d$ there exists a \emph{linear map} $\Pi: \R^d \rightarrow \R^k$ where $k = O\left(\frac{\log n}{\epsilon^2}\right)$ such that \emph{for all $i,j$},
		\begin{align*}
			(1-\epsilon)\|\bv{q}_i - \bv{q}_j\|_2 \leq \|\bs{\Pi}\bv{q}_i - \bs{\Pi}\bv{q}_j\|_2 \leq (1+\epsilon)\|\bv{q}_i - \bv{q}_j\|_2.
		\end{align*}
	\end{lemma}
	\begin{center}
		\includegraphics[height=.45\textheight]{jl_sketch.png}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{euclidean dimensionality reduction}
	\begin{center}
		Remarkably, $\Pi$ can be chosen \emph{completely at random}!
	\end{center}
	\textbf{One possible construction:} Random Gaussian.
	\begin{align*}
		\bs{\Pi}_{i,j} = \frac{1}{\sqrt{k}} \mathcal{N}(0,1)
	\end{align*}
	The map $\bs{\Pi}$ is \textbf{\alert{oblivious to the data set}}. This stands in contrast to e.g. PCA, amoung other differences.
	
	[Indyk, Motwani 1998] [Arriage, Vempala 1999] [Achlioptas 2001] [Dasgupta, Gupta 2003].
	
	Many other possible choices suffice -- you can use random $\{+1,-1\}$ variables, sparse random matrices, pseudorandom $\Pi$. Each with different advantages. 
\end{frame}

\begin{frame}
	\frametitle{randomized jl constructions}
	\begin{center}
		Let $\bs{\Pi} \in \R^{k\times d}$ be chosen so that each entry equals $\frac{1}{\sqrt{k}}  \mathcal{N}(0,1)$.
		
		... or each entry equals $\frac{1}{\sqrt{k}}  \pm 1$ with equal probability.
	\end{center}
	\vspace{1em}
	
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\includegraphics[width=\textwidth]{rand_gauss.png}
		\end{column}
		\begin{column}{0.5\textwidth}
			\includegraphics[width=\textwidth]{rand_sign.png}
		\end{column}
	\end{columns}
	
	\begin{center}
		A random orthogonal matrix also works. I.e. with $\Pi\Pi^T = \bv{I}_{k\times k}$. For this reason, the JL operation is often called a ``random projection", even though it technically isn't a projection when entries are i.i.d.
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{random projection}
	\begin{center}
		\includegraphics[width=.6\textwidth]{random_projection.png}
	\end{center}
	Intuitively, close points will remain close after projection, and far points will remain far. 
\end{frame}

\begin{frame}
	\frametitle{euclidean dimensionality reduction}
	\textbf{Intermediate result:}
	\begin{lemma}[Distributional JL Lemma]
		Let $\bs{\Pi} \in \R^{k\times d}$ be chosen so that each entry equals $\frac{1}{\sqrt{k}}  \mathcal{N}(0,1)$, where $\mathcal{N}(0,1)$ denotes a standard Gaussian random variable. 
		
		If we choose $k = O\left(\frac{\log(1/\delta)}{\epsilon^2}\right)$, then for \emph{any vector $\bv{x}$}, with probability $(1-\delta)$:
		\begin{align*}
			(1-\epsilon)\|\bv{x}\|_2^2 \leq \|\bs{\Pi}\bv{x}\|_2^2 \leq (1+\epsilon) \|\bv{x}\|_2^2
		\end{align*}
	\end{lemma}
	
	\begin{center}\alert{
			\textbf{Given this lemma, how do we prove the traditional Johnson-Lindenstrauss lemma?}}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{jl from distributional jl}
	We have a set of vectors $\bv{q}_1, \ldots, \bv{q}_n$. Fix $i,j \in 1,\ldots, n$. 
	
	Let $\bv{x} = \bv{q}_i - \bv{q}_j$. By linearity, $\bs{\Pi}\bv{x} = \bs{\Pi}(\bv{q}_i - \bv{q}_j) = \bs{\Pi}\bv{q}_i - \bs{\Pi}\bv{q}_j$.
	
	By the Distributional JL Lemma, with probability $1-\delta$,
	\begin{align*}
		(1-\epsilon)\|\bv{q}_i - \bv{q}_j\|_2 \leq \|\bs{\Pi}\bv{q}_i - \bs{\Pi}\bv{q}_j\|_2 \leq (1+\epsilon) \|\bv{q}_i - \bv{q}_j\|_2.
	\end{align*}	
\end{frame}

\begin{frame}
	\frametitle{jl from distributional jl}
	By the Distributional JL Lemma, with probability $1-\delta$, we have that for any $\bv{q}_i$, $\bv{q}_j$,
	\begin{align*}
		(1-\epsilon)\|\bv{q}_i - \bv{q}_j\|_2 \leq \|\bs{\Pi}\bv{q}_i - \bs{\Pi}\bv{q}_j\|_2 \leq (1+\epsilon) \|\bv{q}_i - \bv{q}_j\|_2.
	\end{align*}
	Finally, set $\delta = \frac{1}{10n^2}$. Since there are $< n^2$ total $i,j$ pairs, by a union bound we have that with probability $9/10$, the above will hold \emph{for all} $i,j$, as long as we compress to:
	
	\begin{align*}
		k = O\left(\frac{\log(1/(1/n^2))}{\epsilon^2}\right) = O\left(\frac{\log n}{\epsilon^2}\right) \text{ dimensions.}\qed
	\end{align*}
	
\end{frame}

\begin{frame}
	\frametitle{proof of distributional jl}
	Want to argue that, with probability $(1-\delta)$,
	\begin{align*}
		(1-\epsilon)\|\bv{x}\|_2^2 \leq |\bs{\Pi}\bv{x}\|_2^2 \leq (1+\epsilon)\|\bv{x}\|_2^2 
	\end{align*}
	
	\begin{center}
		\alert{\textbf{Claim}: $\E \left[\|\bs{\Pi} \bv{x} \|_2^2\right] = \|\bv{x}\|_2^2.$}
	\end{center}
	
	\vspace{-1em}
	Some notation:
	\begin{center}
		\includegraphics[width=.6\textwidth]{jl_notation.png}
		
		So each $\bs{\pi}_i$ contains $\mathcal{N}(0,1)$ entries. 
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{proof of distributional jl}
	\text{Intermediate Claim:}
	\begin{align*}
		\E\left[\|\bs{\Pi} \bv{x} \|_2^2 \right]  = \E\left[\left(\langle\bs{\pi}_1,\bv{x}\rangle\right)^2 \right] .
	\end{align*}
	%	\begin{align*}
		%		\|\bs{\Pi} \bv{x} \|_2^2 = \sum_i^k \bv{s}[i]^2 = \sum_i^k \left(\frac{1}{\sqrt{k}}\langle\bs{\pi}_i,\bv{x}\rangle\right)^2 = \frac{1}{k}\sum_i^k \left(\langle\bs{\pi}_i,\bv{x}\rangle\right)^2 
		%	\end{align*}
	%	\begin{align*}
		%		\E\left[\|\bs{\Pi} \bv{x} \|_2^2 \right] &= \frac{1}{k}\sum_i^k \E\left[\left(\langle\bs{\pi}_i,\bv{x}\rangle\right)^2 \right] \\
		%		& =\E\left[\left(\langle\bs{\pi}_i,\bv{x}\rangle\right)^2 \right] 
		%	\end{align*}
	\vspace{9em}
	\begin{block}{\vspace*{-3ex}}
		\small \textbf{Goal}: Prove $\E \left[\|\bs{\Pi} \bv{x} \|_2^2\right] = \|\bv{x}\|_2^2$.
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{proof of distributional jl}	
	\begin{align*}
		\langle\bs{\pi}_1,\bv{x}\rangle = Z_1\cdot\bv{x}(1) + Z_2\cdot\bv{x}(2)  +  \ldots + Z_d\cdot\bv{x}(d)
	\end{align*}
	where each $Z_1, \ldots, Z_d$ is a standard normal $\mathcal{N}(0,1)$ random variable. 
	
	We have that $Z_i \cdot\bv{x}(i)$ is a normal $\mathcal{N}(0,\bv{x}(i)^2)$ random variable.
	
	\vspace{5em}
	\begin{block}{\vspace*{-3ex}}
		\small \textbf{Goal}: Prove $\E \left[\|\bs{\Pi} \bv{x} \|_2^2\right] = \|\bv{x}\|_2^2$. Have: $\E \left[\|\bs{\Pi} \bv{x} \|_2^2\right] = \E\left[\left(\langle\bs{\pi}_1,\bv{x}\rangle\right)^2 \right]$
	\end{block}
\end{frame}

\begin{frame}[t]
	\frametitle{stable random variables}
	\textbf{What type of random variable is $\langle\bs{\pi}_i,\bv{x}\rangle$?}
	\begin{fact}[Stability of Gaussian random variables]
		\begin{align*}
			\mathcal{N}(\mu_1, \sigma_1^2) + \mathcal{N}(\mu_2, \sigma_2^2) =  \mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
		\end{align*}
	\end{fact}
\vspace{-1.5em}

	\begin{align*}
		\langle\bs{\pi}_1,\bv{x}\rangle &= \mathcal{N}(0,\bv{x}(1)^2) + \mathcal{N}(0,\bv{x}(2)^2) + \ldots + \mathcal{N}(0,\bv{x}(d)^2) \\ &= \mathcal{N}(0,\|\bv{x}\|_2^2). 
	\end{align*}
	
	So $\E \left[\|\bs{\Pi} \bv{x} \|_2^2\right] = \E\left[\left(\langle\bs{\pi}_1,\bv{x}\rangle\right)^2 \right] = \|\bv{x}\|_2^2$, as desired.
	
		\vspace{2em}
	\begin{block}{\vspace*{-3ex}}
		\small \textbf{Goal}: Prove $\E \left[\|\bs{\Pi} \bv{x} \|_2^2\right] = \|\bv{x}\|_2^2$. Have: $\E \left[\|\bs{\Pi} \bv{x} \|_2^2\right] = \E\left[\left(\langle\bs{\pi}_1,\bv{x}\rangle\right)^2 \right]$
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{proof of distributional jl}
	Want to argue that, with probability $(1-\delta)$,
	\begin{align*}
		(1-\epsilon)\|\bv{x}\|_2^2 \leq \|\bs{\Pi}\bv{x}\|_2^2 \leq (1+\epsilon)\|\bv{x}\|_2^2 
	\end{align*}
	
	\begin{enumerate}
		\item $\E \left[\|\bs{\Pi} \bv{x} \|_2^2\right] = \|\bv{x}\|_2^2$.
		\item Need to use a concentration bound.
	\end{enumerate}
	\begin{align*}
		\|\bs{\Pi} \bv{x} \|_2^2 = \frac{1}{k}\sum_{i=1}^k \left(\langle\bs{\pi}_i,\bv{x}\rangle\right)^2 = \frac{1}{k}\sum_{i=1}^k \mathcal{N}(0,\|\bv{x}\|_2^2)
	\end{align*}
	\alert{``Chi-squared random variable with $k$ degrees of freedom.''}
\end{frame}

\begin{frame}[t]
	\frametitle{concentration of chi-squared random variables}
	\begin{lemma} Let $Z$ be a Chi-squared random variable with $k$ degrees of freedom. 
		\begin{align*}
			\Pr[|\E Z - Z| \geq \epsilon \E Z] \leq 2 e^{-k\epsilon^2/8}
		\end{align*}
	\end{lemma}
	
	\vspace{8em}
	\begin{block}{\vspace*{-3ex}}
		\small \textbf{Goal}: Prove $\|\bs{\Pi} \bv{x} \|_2^2$ concentrates within $1 \pm \epsilon$ of its expectation, which equals $\|\bv{x} \|_2^2$.
	\end{block}
\end{frame}







\begin{frame}
	\frametitle{connection to last lecture}
	If high dimensional geometry is so different from low-dimensional geometry, why is \emph{dimensionality reduction possible?} Doesn't Johnson-Lindenstrauss tell us that high-dimensional geometry can be approximated in low dimensions?
\end{frame}

\begin{frame}
	\frametitle{connection to dimensionality reduction}
	\textbf{Hard case:} $\bv{x}_1, \ldots, \bv{x}_n \in \R^d$ are all mutually orthogonal unit vectors: 
	\begin{align*}
		\|\bv{x}_i - \bv{x}_j\|_2^2 &= 2 & &\text{for all $i,j$.}  
	\end{align*}
	
	From our result earlier, in $O(\log n /\epsilon^2)$ dimensions, there exists $2^{O(\epsilon^2\cdot \log n /\epsilon^2)} \geq n $ unit vectors that are close to mutually orthogonal.
	
	$O(\log n /\epsilon^2)$ = \emph{just enough} dimensions. 
\end{frame}


%\begin{frame}[standout]
%	\begin{center}
	%		break
	%	\end{center}
%\end{frame}
%
\begin{frame}
	\frametitle{dimensionality reduction}
	\begin{center}
		The Johnson-Lindenstrauss Lemma let us sketch vectors and preserve their $\ell_2$ Euclidean distance. 
		
		We also have dimensionality reduction techniques that preserve alternative measures of similarity!
	\end{center}
\end{frame}


\begin{frame}
	\frametitle{jaccard similarity}
	Another distance measure (actually a similarity measure) between \emph{binary} vectors in $\{0,1\}^d$ :
	\begin{definition}[Jaccard Similarity]
		\begin{align*}
			J(\bv{q},\bv{y}) = \frac{|\bv{q} \cap \bv{y}|}{|\bv{q} \cup \bv{y}|} = \frac{\text{\# of non-zero entries in common}}{\text{total \# of non-zero entries}}
		\end{align*}
		Natural similarity measure for binary vectors. $0\leq J(\bv{q},\bv{y})\leq 1$.
	\end{definition}
	
	Can be applied to any data which has a natural binary representation (more than you might think). 
	\begin{align*}
		\bv{y} = \begin{bmatrix}1 & 0 & 1 & 1 & 0 & 0\end{bmatrix}\\
		\bv{q} = \begin{bmatrix}1 & 1 & 0 & 1 & 0 & 0\end{bmatrix}
	\end{align*}
\end{frame}

\begin{frame}
	\frametitle{similarity estimation}
	\begin{center}
		How does \textbf{Shazam} match a song clip against a library of 8 million songs (32 TB of data) in a fraction of a second?
	\end{center}
	\vspace{-1em}
		\begin{figure}[h]
			\centering
			\begin{subfigure}[t]{0.35\textwidth}
				\centering
				\includegraphics[width=\textwidth]{spectrogram.png}
				\caption{Spectrogram extracted from audio clip.}
			\end{subfigure}
			~
			\begin{subfigure}[t]{0.35\textwidth}
				\centering
				\includegraphics[width=\textwidth]{spectrogramThresh.png}
				\caption{Processed spectrogram: used to construct audio ``fingerprint'' $\textbf{q}\in \{0,1\}^d$.}
			\end{subfigure}
		\end{figure}
		
		\vspace{-.5em}
			Each clip is represented by a high dimensional binary vector $\bv{q}$.
			\includegraphics[width=\textwidth]{binaryVector.png}
\end{frame}

\begin{frame}
	\frametitle{similarity estimation}
	\begin{center}
		Given $\textbf{q}$, find any nearby ``fingerprint'' $\bv{y}$  in a database -- i.e. any $\bv{y}$ with $\text{dist}(\bv{y}, \bv{q})$ small. 
	\end{center}
	
	\textbf{Challenges}:
	\begin{itemize}
		\item Database is possibly huge: $O(nd)$ bits.
		\item Expensive to compute $\text{dist}(\bv{y},\bv{q})$: $O(d)$ time.
	\end{itemize}
\end{frame}

%\begin{frame}
%	\frametitle{similarity estimation}
%	\textbf{Goal:} Design a more compact sketch for comparing $\bv{q}, \bv{y}\in \{0,1\}^d$. Ideally $\ll d$ space/time complexity.
%	\begin{align*}
%		C(\textbf{q}) \in \R^k \\	
%		C(\textbf{y}) \in \R^k 
%	\end{align*}
%	\begin{center}
%		\vspace{-.5em}
%		\includegraphics[width=.8\textwidth]{compression.png}
%		\vspace{-.5em}
%	\end{center}
%	\textbf{Homomorphic Compression:}
%	
%	$C(\bv{q})$ should be similar to $C(\bv{y})$ if $\bv{q}$ is similar to $\bv{y}$.
%\end{frame}
%
%\begin{frame}
%	\frametitle{jaccard similarity}
%	\begin{definition}[Jaccard Similarity]
%		\begin{align*}
%			J(\bv{q},\bv{y}) = \frac{|\bv{q} \cap \bv{y}|}{|\bv{q} \cup \bv{y}|} = \frac{\text{\# of non-zero entries in common}}{\text{total \# of non-zero entries}}
%		\end{align*}
%		Natural similarity measure for binary vectors. $0\leq J(\bv{q},\bv{y})\leq 1$.
%	\end{definition}
%	
%	Can be applied to any data which has a natural binary representation (more than you might think). 
%\end{frame}

\begin{frame}
	\frametitle{jaccard similarity for document comparison}
	\textbf{``Bag-of-words'' model:}
	\begin{center}
		\includegraphics[width=.95\textwidth]{bagofwords.png}
	\end{center}
	
	How many words do a pair of documents have in common?
\end{frame}

\begin{frame}
	\frametitle{jaccard similarity for document comparison}
	\textbf{``Bag-of-words'' model:}
	\begin{center}
		\includegraphics[width=.95\textwidth]{bigrams.png}
	\end{center}
	
	How many bigrams do a pair of documents have in common?
\end{frame}

%\begin{frame}
%	\frametitle{jaccard similarity for seismic data}
%	\begin{center}
	%		\vspace{-.5em}
	%		\includegraphics[width=.6\textwidth]{earthquakeFeatures.jpg}
	%		
	%		\vspace{-.5em}
	%		Feature extract pipeline for earthquake data.
	%		
	%		(see paper by Rong et al. posted on course website)
	%	\end{center}
%\end{frame}

\begin{frame}
	\frametitle{applications: document similarity}
	\begin{itemize}
		\item Finding duplicate or new duplicate documents or webpages.
		\item Change detection for high-speed web caches.
		\item Finding near-duplicate emails or customer reviews which could indicate spam.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{jaccard similarity for seismic data}
	\begin{center}
			\vspace{-.5em}
			\includegraphics[width=.6\textwidth]{earthquakeFeatures.jpg}
			
			\vspace{-.5em}
			Feature extract pipeline for earthquake data.
			
			(see paper by Rong et al. posted on course website)
		\end{center}
\end{frame}


\begin{frame}
	\frametitle{similarity estimation}
	\textbf{Goal:} Design a compact sketch $C: \{0,1\}\rightarrow \R^k$:
	\begin{center}
		\vspace{-.5em}
		\includegraphics[width=.8\textwidth]{compression.png}
		\vspace{-.5em}
	\end{center}
	\textbf{Homomorphic Compression:} 
	Want to use $C(\bv{q}), C(\bv{y})$ to approximately compute the Jaccard similarity $J(\bv{q},\bv{y})$.
\end{frame}


%	\textbf{Other applications:}
%\begin{itemize}
%	\item Change detection in documents (high speed web caches).
%	\item Analyzing seismic data (matching signatures of earthquakes).
%	\item User recommendations on social networking sites.
%\end{itemize}

\begin{frame}
	\frametitle{minhash}
	\textbf{MinHash (Broder, '97)}:
	\begin{itemize}
	\item Choose $k$ random hash functions $h_1, \ldots, h_k: \{1,\ldots, n\} \rightarrow [0,1]$. 
	\item For $i\in 1, \ldots,k$, 
	\begin{itemize}
		\item Let $c_i = \min_{j, \bv{q}_j = 1} h_i(j)$.
	\end{itemize}
	\item $C(\bv{q}) = [c_1, \ldots, c_k]$.
	\end{itemize}
	\begin{center}
			\includegraphics[width=\textwidth]{minHash1.png}	
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{minhash}
	\begin{itemize}
		\item Choose $k$ random hash functions $h_1, \ldots, h_k: \{1,\ldots, n\} \rightarrow [0,1]$. 
		\item For $i\in 1, \ldots,k$, 
		\begin{itemize}
			\item Let $c_i = \min_{j, \bv{q}_j = 1} h_i(j)$.
		\end{itemize}
		\item $C(\bv{q}) = [c_1, \ldots, c_k]$.
	\end{itemize}
	\begin{center}
		\includegraphics[width=\textwidth]{minHash2.png}	
	\end{center}
\end{frame}

\begin{frame}[t]
	\frametitle{minhash analysis}
	\textbf{Claim:} $\Pr[c_i(\bv{q}) = c_i(\bv{y})] = J(\bv{q},\bv{y})$.	
	\begin{center}
		\includegraphics[width=.8\textwidth]{minHashSimple.png}
	\end{center}
	\textbf{Proof:} 
	
	1. For $c_i(\bv{q}) = c_i(\bv{y})$, we need that $\argmin_{i\in \bv{q}} h(i) = \argmin_{i\in \bv{y}} h(i)$.
\end{frame}

\begin{frame}[t]
	\frametitle{minhash analysis}
	\textbf{Claim:} $\Pr[c_i(\bv{q}) = c_i(\bv{y})] = J(\bv{q},\bv{y})$.	
	\begin{center}
		\includegraphics[width=.8\textwidth]{minhash_colored.png}
	\end{center}
	2. Every non-zero index in $\bv{q}\cup \bv{y}$ is equally likely to produce the lowest hash value.
	$c_i(\bv{q}) = c_i(\bv{y})$ only if this index is 1 in \emph{both} $\bv{q}$ and $\bv{y}$. There are $\bv{q}\cap \bv{y}$ such indices. So:
	\begin{align*}
		\Pr[c_i(\bv{q}) =c_i(\bv{y})]= \frac{\bv{q}\cap \bv{y}}{\bv{q}\cup \bv{y}}  = J(\bv{q},\bv{y})
	\end{align*}
\end{frame}


\begin{frame}
	\frametitle{minhash analysis}
	Let $J = J(\bv{q},\bv{y})$ denote the Jaccard similarity between $\bv{q}$ and $\bv{y}$. \vspace{1em}
	
	\textbf{Return:} $\tilde{J} = \frac{1}{k} \sum_{i=1}^k \mathbbm{1}[c_i(\bv{q}) = c_i(\bv{y})]$. 
	
	\textbf{Unbiased estimate for Jaccard similarity:}
	\begin{align*}
		\E \tilde{J} = \hspace{14em}
	\end{align*}
	
	\begin{center}
		\includegraphics[width=.6\textwidth]{minHashCompare.png}
	\end{center}
	The more repetitions, the lower the variance. 
\end{frame}

\begin{frame}[t]
	\frametitle{minhash analysis}
	Let $J = J(\bv{q},\bv{y})$ denote the true Jaccard similarity.
	
	\textbf{Estimator:} $\tilde{J} = \frac{1}{k} \sum_{i=1}^k \mathbbm{1}[c_i(\bv{q}) = c_i(\bv{y})]$. 
	\begin{align*}
		\Var [\tilde{J}] =\hspace{16em}
	\end{align*}
	
	Plug into Chebyshev inequality. How large does $k$ need to be so that with probability $> 1 - \delta$, $|J-\tilde{J}| \leq \epsilon$?
\end{frame}

\begin{frame}
	\frametitle{minhash analysis}
	\textbf{Chebyshev inequality:} As long as $\alert{{k = O\left(\frac{1}{\epsilon^2\delta}\right)}}$, then with prob. $1-\delta$,
	\begin{align*}
		J(\bv{q}, \bv{y}) -\epsilon \leq \tilde{J}\left(C(\bv{q}),C(\bv{y})\right)   \leq J(\bv{q}, \bv{y}) + \epsilon. 
	\end{align*}
	\begin{center}
		And $\tilde{J}$ only takes $O(k)$ time to compute! \alert{\textbf{Independent}} of original fingerprint dimension $d$.
	\end{center}	
	
	Can be improved to $\log(1/\delta)$ dependence. Can anyone tell me how?
\end{frame}



\begin{frame}
	\frametitle{similarity sketching}
	\includegraphics[width=\textwidth]{sketch_paradigm.png}
\end{frame}

\begin{frame}[standout]
	\begin{center}
		break
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{near neighbor search}
	\textbf{Common goal:} Find all vectors in database $\bv{q}_1, \ldots, \bv{q}_n \in \R^d$ that are close to some input query vector $\bv{y}\in \R^d$. I.e. find all of $\bv{y}$'s ``nearest neighbors'' in the database.
	\begin{itemize}
		\item The Shazam problem.
		\item Audio + video search.
		\item Finding duplicate or near duplicate documents.
		\item Detecting seismic events.
	\end{itemize}

		\begin{center}
	\textbf{\alert{How does similarity sketching help in these applications?}}
		\end{center}
	\begin{itemize}
		\item Improves runtime of ``linear scan'' from $O(nd)$ to $O(nk)$.
		\item Improves space complexity from $O(nd)$ to $O(nk)$. This can be super important -- e.g. if it means the linear scan only accesses vectors in fast memory.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{beyond a linear scan}
	\begin{center}
		\textbf{New goal:} \emph{Sublinear} $o(n)$ time to find near neighbors. 
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{beyond a linear scan}	
	This problem can already be solved for a small number of dimensions using space partitioning approaches (e.g. kd-tree).
	
	\includegraphics[height=.4\textheight]{kdtree.png}	\includegraphics[height=.4\textheight]{3dtree.png}
	
	Runtime is roughly $O(d\cdot \min(n,2^d))$, which is only sublinear for $d = o(\log n)$.

\end{frame}

\begin{frame}
	\frametitle{high dimensional near neighbor search}	
	Only been attacked much more recently:
	\begin{itemize}
		\item \textbf{\alert{Locality-sensitive hashing [Indyk, Motwani, 1998]}}
		\item Spectral hashing [Weiss, Torralba, and Fergus, 2008]
		\item Vector quantization [J\'{e}gou, Douze, Schmid, 2009]
		\begin{itemize}
			\item Probably the most practical. This is most similar to the custom method e.g. Shazam uses.
		\end{itemize}
	\end{itemize}

\textbf{Key Insight:} Trade worse space-complexity for better time-complexity. 
\end{frame}

\begin{frame}
	\frametitle{locality sensitive hash functions}
	Let $h: \R^d \rightarrow \{1, \ldots, m\}$ be a random hash function. 
	
	We call $h$ \emph{locality sensitive} for similarity function $s(\bv{q},\bv{y})$ if $\Pr\left[h(\bv{q}) == h(\bv{y})\right]$ is:
	\begin{itemize}
		\item Higher when $\bv{q}$ and $\bv{y}$ are more similar, i.e. $s(\bv{q},\bv{y})$ is higher.
		\item Lower when $\bv{q}$ and $\bv{y}$ are more dissimilar, i.e. $s(\bv{q},\bv{y})$ is lower. 
	\end{itemize}
\begin{center}
	\includegraphics[width=.9\textwidth]{cam_lsh.png}
\end{center}
\end{frame}

\begin{frame}
	\frametitle{locality sensitive hash functions}
	LSH for $s(\bv{q},\bv{y})$  equal to Jaccard similarity:
	\begin{itemize}
		\item Let $c: \{0,1\}^d \rightarrow [0,1]$ be a single instantiation of MinHash.
		\item Let $g: [0,1] \rightarrow \{1, \ldots, m\}$ be a uniform random hash function.
		\item Let $h(\bv{q}) = g(c(\bv{q})).$
	\end{itemize}
\begin{center}
	\includegraphics[width=.7\textwidth]{single_min_hash.png}
\end{center}

\end{frame}

\begin{frame}
	\frametitle{locality sensitive hash functions}
	LSH for Jaccard similarity:
	\begin{itemize}
		\item Let $c: \{0,1\}^d \rightarrow [0,1]$ be a single instantiation of MinHash.
		\item Let $g: [0,1] \rightarrow \{1, \ldots, m\}$ be a uniform random hash function.
		\item Let $h(\bv{x}) = g(c(\bv{x})).$
	\end{itemize}
		If $J(\bv{q},\bv{y}) = v$, 
		\begin{align*}
			\Pr\left[h(\bv{q}) == h(\bv{y})\right] = \hspace{15em}
		\end{align*}
\end{frame}

\begin{frame}
	\frametitle{near neighbor search}
	\begin{center}
	Basic approach for near neighbor search in a database.
	\end{center}
		\textbf{Pre-processing:}
	\begin{itemize}
		\item Select random LSH function $h: \{0,1\}^d \rightarrow 1,\ldots, m$.
		\item Create table $T$ with $m = O(n)$ slots.\footnote{Enough to make the $O(1/m)$ term negligible.}
		\item For $i = 1,\ldots, n$, insert $\bv{q}_i$ into $T(h(\bv{q}_i))$.
	\end{itemize}
	\textbf{Query:}
	\begin{itemize}
		\item Want to find near neighbors of input $\bv{y}\in\{0,1\}^d$.
		\item Linear scan through all vectors $\bv{q}\in T(h(\bv{y}))$ and return any that are close to $\bv{y}$. Time required is $O(d\cdot |T(h(\bv{y})|)$.
	\end{itemize}
\vspace{1em}
\end{frame}

\begin{frame}
	\frametitle{near neighbor search}
	\begin{center}
		\includegraphics[width=.8\textwidth]{basicScheme.png}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{near neighbor search}
	\textbf{Two main considerations:}
	\begin{itemize}
		\item \textbf{False Negative Rate}: What's the probability we do not find a vector that \emph{is close} to $\bv{y}$?
		\item \textbf{False Positive Rate}: What's the probability that a vector in $T(h(\bv{y}))$ \emph{is not close} to $\bv{y}$?
	\end{itemize}

A higher false negative rate means we miss near neighbors.

A higher false positive rate means increased runtime -- we need to compute $J(\bv{q},\bv{y})$ for every $\bv{q}\in T(h(\bv{y}))$ to check if it's actually close to $\bv{y}$.

\textbf{Note:} The meaning of ``close'' and ``not close'' is application dependent. E.g. we might specify that we want to find anything with Jaccard similarity $> .4$, but not with Jaccard similarity $< .2$. 
\end{frame}

\begin{frame}
	\frametitle{reducing false negative rate}	
	Suppose the nearest database point $\bv{q}$ has $J(\bv{y},\bv{q}) = .4$.
	\begin{center}
	\textbf{\alert{What's the probability we do not find $\bv{q}$?}}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{reducing false negative rate}
	\begin{center}
		\includegraphics[width=.8\textwidth]{many_tables.png}
	\end{center}
	\textbf{Pre-processing:}
	\begin{itemize}
		\item Select $t$ independent LSH's $h_1, \ldots, h_t: \{0,1\}^d \rightarrow 1,\ldots, m$.
		\item Create tables $T_1, \ldots, T_t$, each with $m$ slots. 
		\item For $i = 1,\ldots, n$, $j = 1,\ldots, t$, 
		\begin{itemize}
			\item Insert $\bv{q}_i$ into $T_j(h_j(\bv{q}_i))$.
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{reducing false negative rate}
	\textbf{Query:}
	\begin{itemize}
		\item Want to find near neighbors of input $\bv{y}\in\{0,1\}^d$.
		\item Linear scan through all vectors in $T_1(h_1(\bv{y}))\cup T_2(h_2(\bv{y}))\cup \ldots, T_t(h_t(\bv{y}))$.
	\end{itemize}

\vspace{2em}
	Suppose the nearest database point $\bv{q}$ has $J(\bv{y},\bv{q}) = .4$.
	\begin{center}
		\textbf{\alert{What's the probability we find $\bv{q}$?}}
	\end{center}
\end{frame}

\begin{frame}[t]
	\frametitle{what happens to false positives?}
	Suppose there is some other database point $\bv{z}$ with $J(\bv{y},\bv{z}) = .2$. 
	
	What is the probability we will need to compute $J(\bv{z},\bv{y})$ in our hashing scheme with one table? I.e. the probability that $\bv{y}$ hashes into at least one bucket containing $\bv{z}$. 
	
	\vspace{4em}
	{\textbf{\alert{In the new scheme with $t=10$ tables?}}}
	
\end{frame}

\begin{frame}[t]
	\frametitle{reducing false positives}
	\small
	\vspace{-.5em}
	\begin{center}
	\textbf{Change our locality sensitive hash function}.
	\vspace{-.5em}
	\end{center}
	\emph{Tunable} LSH for Jaccard similarity:
	\vspace{-.5em}
	\begin{itemize}
		\item Choose parameter $r \in \mathbb{Z}^+$.
		\item Let $c_1, \ldots, c_r: \{0,1\}^d \rightarrow [0,1]$ be random MinHash.
		\item Let $g: [0,1]^r \rightarrow \{1, \ldots, m\}$ be a uniform random hash function.
		\item Let $h(\bv{x}) = g(c_1(\bv{x}), \ldots, c_r(\bv{x})).$
	\end{itemize}
\vspace{-1em}
	\begin{center}
			\includegraphics[width=.8\textwidth]{banded_hash.png}
	\end{center}


\end{frame}

\begin{frame}[t]
	\frametitle{reducing false positives}
	\small
	\emph{Tunable} LSH for Jaccard similarity:
	\begin{itemize}
		\item Choose parameter $r \in \mathbb{Z}^+$.
		\item Let $c_1, \ldots, c_r: \{0,1\}^d \rightarrow [0,1]$ be random MinHash.
		\item Let $g: [0,1]^r \rightarrow \{1, \ldots, m\}$ be a uniform random hash function.
		\item Let $h(\bv{x}) = g(c_1(\bv{x}), \ldots, c_r(\bv{x})).$
	\end{itemize}
	
	If $J(\bv{q},\bv{y}) = v$, then $\Pr\left[h(\bv{q}) == h(\bv{y})\right] = $
	
\end{frame}

\begin{frame}[t]
	\frametitle{tunable lsh}
	\begin{center}
		\includegraphics[width=.8\textwidth]{tuning_minhash.png}
	\end{center}
\end{frame}

\begin{frame}[t]
	\frametitle{tunable lsh}
	Full LSH cheme has two parameters to tune:
	\begin{center}
		\includegraphics[width=.8\textwidth]{full_scheme.png}
	\end{center}
\end{frame}

\begin{frame}[t]
	\frametitle{tunable lsh}
	Effect of \textbf{increasing number of tables} $t$ on:
	\begin{center}
		False Negatives \hspace{6em} False Positives
	\end{center}
\vspace{4em}
	Effect of \textbf{increasing number of bands} $r$ on:
\begin{center}
	False Negatives \hspace{6em} False Positives
\end{center}
\end{frame}

%\begin{frame}
%	\frametitle{some examples}
%	Choose tables $t$ large enough so false negative rate to $1\%$.
%	\begin{center}
%		\alert{\textbf{Parameter:} $\mathbf{r = 1}$.}
%	\end{center}
%	Chance we find $\bv{q}$ with $J(\bv{y},\bv{q}) = .8$:
%	\vspace{5em}
%	
%	Chance we need to check $\bv{z}$ with $J(\bv{y},\bv{z}) = .4$:
%\end{frame}
%
%\begin{frame}
%	\frametitle{some examples}
%	Choose tables $t$ large enough so false negative rate to $1\%$.
%	\begin{center}
%		\alert{\textbf{Parameter:} $\mathbf{r = 2}$.}
%	\end{center}
%	Chance we find $\bv{q}$ with $J(\bv{y},\bv{q}) = .8$:
%	\vspace{5em}
%	
%	Chance we need to check $\bv{z}$ with $J(\bv{y},\bv{z}) = .4$:
%\end{frame}
%
%\begin{frame}
%	\frametitle{some examples}
%	Choose tables $t$ large enough so false negative rate to $1\%$.
%	\begin{center}
%		\alert{\textbf{Parameter:} $\mathbf{r = 5}$.}
%	\end{center}
%	Chance we find $\bv{q}$ with $J(\bv{y},\bv{q}) = .8$:
%	\vspace{5em}
%	
%	Chance we need to check $\bv{z}$ with $J(\bv{y},\bv{z}) = .4$:
%\end{frame}

\begin{frame}
	\frametitle{$s$-curve tuning}
	Probability we check $\bv{q}$ when querying $\bv{y}$ if $J(\bv{q},\bv{y}) = v$:
	\begin{align*}
%	\approx 	1 - (1 - v^r)^t
	\end{align*}
	\begin{center}
		\includegraphics[width=.6\textwidth]{scurve_5_5.png}
		
		$r = 5, t = 5$
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{$s$-curve tuning}
	Probability we check $\bv{q}$ when querying $\bv{y}$ if $J(\bv{q},\bv{y}) = v$:
	\begin{align*}
	\approx 1 - (1 - v^r)^t
	\end{align*}
	\begin{center}
		\includegraphics[width=.6\textwidth]{scurve_5_40.png}
		
		$r = 5, t = 40$
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{$s$-curve tuning}
	Probability we check $\bv{q}$ when querying $\bv{y}$ if $J(\bv{q},\bv{y}) = v$:
	\begin{align*}
	\approx 1 - (1 - v^r)^t 
	\end{align*}
	\begin{center}
		\includegraphics[width=.6\textwidth]{scurve_40_5.png}
		
		$r = 40, t = 5$
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{$s$-curve tuning}
	Probability we check $\bv{q}$ when querying $\bv{y}$ if $J(\bv{q},\bv{y}) = v$:
	\begin{align*}
	1 - (1 - v^r)^t
	\end{align*}
	\begin{center}
		\includegraphics[width=.6\textwidth]{scurve_centered.png}
		
		Increasing both $r$ and $t$ gives a steeper curve. 
		
		\alert{\textbf{Better for search, but worse space complexity}.}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{fixed threshold}
	\small
	\textbf{Use Case 1:} Fixed threshold.
	\begin{itemize}
		\item Shazam wants to find match to audio clip $\bv{y}$ in a database of 10 million  clips.
		\item There are 10 \emph{true matches} with $J(\bv{y},\bv{q}) > .9$.
		\item There are 10,000 \emph{near matches} with $J(\bv{y},\bv{q}) \in [.7,.9]$.
		\item All other items have $J(\bv{y},\bv{q}) < .7$.
	\end{itemize}
	With $r = 25$ and $t = 40$, 
	\begin{itemize}
		\item Hit probability for $J(\bv{y},\bv{q}) > .9$ is $\gtrsim 1 - (1 - .9^{25})^{40} = .95$
		\item Hit probability for $J(\bv{y},\bv{q}) \in [.7,.9]$ is $\lesssim 1 - (1 - .9^{25})^{40} = .95$
		\item Hit probability for $J(\bv{y},\bv{q}) < .7$ is $\lesssim 1 - (1 - .7^{25})^{40} = .005$
	\end{itemize}
	\textbf{Upper bound on total number of items checked:} 
	\begin{align*}
	.95\cdot 10 + .95 \cdot 10,000 + .005 \cdot 9,989,990 \alert{\approx 60,000 \ll 10,000,000}.
	\end{align*}  
\end{frame}

\begin{frame}
	\frametitle{fixed threshold}
	\begin{center}
	Space complexity: 40 hash tables \alert{$\approx 40\cdot O(n)$}. 
	
	\textbf{Directly trade space for fast search.}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{fixed threshold r}
	\begin{center}
	\textbf{Near Neighbor Search Problem}
\end{center}

	Concrete worst case result:
	\begin{theorem}[Indyk, Motwani, 1998]
		If there exists some $q$ with $\|\bv{q} - \bv{y}\|_0 \leq R$, return a vector $\tilde{\bv{q}}$ with $\|\tilde{\bv{q}} - \bv{y}\|_0 \leq C\cdot R$ in:
		\begin{itemize}
			\item Time: $O\left(n^{1/C}\right)$.
			\item Space: $O\left(n^{1 + 1/C}\right)$. 
		\end{itemize}
	\end{theorem}
	$\|\bv{q} - \bv{y}\|_0 = $ ``hamming distance" = number of elements that differ between $\bv{q}$ and $\bv{y}$. 
\end{frame}

\begin{frame}
	\frametitle{approximate nearest neighbor search}
		\begin{theorem}[Indyk, Motwani, 1998]
		Let $q$ be the closest database vector to $\bv{y}$. Return a vector $\tilde{\bv{q}}$ with $\|\tilde{\bv{q}} - \bv{y}\|_0 \leq C\cdot \|{\bv{q}} - \bv{y}\|_0$ in:
		\begin{itemize}
			\item Time: $\tilde{O}\left(n^{1/C}\right)$.
			\item Space: $\tilde{O}\left(n^{1 + 1/C}\right)$. 
		\end{itemize}
	\end{theorem}
\end{frame}

\begin{frame}
	\frametitle{other lsh functions}
	\begin{center}
	Good locality sensitive hash functions exists for other similarity measures.
	\end{center}
	\textbf{Cosine similarity $\cos\left(\theta(\bv{x},\bv{y})\right) = \frac{\langle \bv{x},\bv{y}\rangle}{\|\bv{x}\|_2\|\bv{y}\|_2}$:}
	\begin{center}
		\includegraphics[width=.7\textwidth]{cos_sim.png}
		
		$-1 \leq \cos\left(\theta(\bv{x},\bv{y})\right) \leq 1$.
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{cosine similarity}
	\begin{center}
		Cosine similarity is natural ``inverse" for Euclidean distance.
	\end{center}
		\textbf{Euclidean distance $\|\bv{x} - \bv{y}\|_2^2$:}
		\begin{itemize}
			\item Suppose for simplicity that $\|\bv{x}\|_2^2 = \|\bv{y}\|_2^2 = 1$.
		\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{simhash}
	Locality sensitive hash for \textbf{cosine similarity}:
	\begin{itemize}
		\item Let $\bv{g} \in \R^d$ be randomly chosen with each entry $\mathcal{N}(0,1)$. 
		\item Let $f: \{-1,1\} \rightarrow \{1,\ldots, m\}$ be a uniformly random hash function. 
		\item $h: \R^d \rightarrow \{1,\ldots, m\}$ is definied $h(\bv{x}) = f\left(\sign(\langle \bv{g}, \bv{x} \rangle)\right)$.
	\end{itemize}
	\begin{center}
		\alert{\textbf{
				\large
				If $\cos(\theta(\bv{x},\bv{y})) = v$, what is $\Pr[h(\bv{x}) == h(\bv{y})]$?
		}}
	\end{center}
\end{frame}


%\begin{frame}
%	\frametitle{simhash}
%	\begin{center}
%		\textbf{Inspired by Johnson-Lindenstrauss sketching}
%		
%		\includegraphics[width=\textwidth]{simhash_jl.png}
%	\end{center}
%\end{frame}



\begin{frame}[t]
	\frametitle{simhash analysis}
	\textbf{Theorem:} If $\cos(\theta(\bv{x},\bv{y})) = v$, then 
	\begin{align*}
		\Pr[h(\bv{x}) == h(\bv{y})] = 1 - \frac{\theta}{\pi}  + O(\frac{1}{m})= 1 - \frac{\cos^{-1}(v)}{\pi} + O(\frac{1}{m})
	\end{align*}
	\begin{center}
		\includegraphics[width=.7\textwidth]{simhash_collision_prob.png}
	\end{center}
Not a linear function in $v$, as we had for MinHash, but still suffices for locality sensitive hashing. 
\end{frame}

\begin{frame}
	\frametitle{simhash}
	SimHash can be tuned, just like our MinHash based LSH function for Jaccard similarity:
	\begin{itemize}
		\item Let $\bv{g}_1,\ldots, \bv{g}_r \in \R^d$ be randomly chosen with each entry $\mathcal{N}(0,1)$. 
		\item Let $f: \{-1,1\}^r \rightarrow \{1,\ldots, m\}$ be a uniformly random hash function. 
		\item $h: \R^d \rightarrow \{1,\ldots, m\}$ is defined $h(\bv{x}) = f\left([\sign(\langle \bv{g}_1, \bv{x} \rangle),\ldots, \sign(\langle \bv{g}_r, \bv{x} \rangle)]\right)$.
	\end{itemize}
\begin{align*}
	\Pr[h(\bv{x}) == h(\bv{y})] = \left(1-\frac{\theta}{\Pi}\right)^r
\end{align*}
\end{frame}

\begin{frame}
	\textbf{To prove:} 
	
	$\Pr[h(\bv{x}) == h(\bv{y})] = 1 - \frac{\theta}{\pi}$,  where $h(\bv{x}) = f\left(\sign(\langle \bv{g}, \bv{x} \rangle)\right)$.
	\frametitle{simhash analysis}
	\vspace{-.5em}
	\begin{center}
		\includegraphics[width=.5\textwidth]{simhash1.png}
	\end{center}
\begin{align*}
	\Pr[h(\bv{x}) == h(\bv{y})] = z + \frac{1-v}{m} \approx z.
\end{align*}
where $z = \Pr[\sign(\langle \bv{g}, \bv{x} \rangle) == \sign(\langle \bv{g}, \bv{y} \rangle)]$
\end{frame}

\begin{frame}
	\frametitle{simhash analysis}
	\vspace{-.5em}
	\begin{center}
		\includegraphics[width=.55\textwidth]{simhash2.png}
	\end{center}
\vspace{-.5em}
$\Pr[h(\bv{x}) == h(\bv{y})] \approx$ probability $\bv{x}$ and $\bv{y}$ are on the same side of hyperplane orthogonal to $\bv{g}$.
\end{frame}



\begin{comment}
\end{comment}

\end{document} 








