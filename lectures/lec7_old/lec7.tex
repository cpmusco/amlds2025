\documentclass[compress]{beamer}

\usetheme[block=fill]{metropolis}

\usepackage{graphicx} % Allows including images
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{color}
\usepackage{xcolor,cancel}
\usepackage{enumitem}
\setitemize{label=\usebeamerfont*{itemize item}%
	\usebeamercolor[fg]{itemize item}
	\usebeamertemplate{itemize item}}
\definecolor{mDarkBrown}{HTML}{604c38}
\definecolor{mDarkTeal}{HTML}{23373b}
\definecolor{mLightBrown}{HTML}{EB811B}
\definecolor{mMediumBrown}{HTML}{C87A2F}
\definecolor{mygreen}{HTML}{98C2B9}
\definecolor{myyellow}{HTML}{DFD79C}
\definecolor{myblue}{HTML}{8CA7CC}
\definecolor{kern}{HTML}{8CC2B7}


\usepackage{float}
\usepackage{framed}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{ulem}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{comment}   
\usepackage{bbm}
\usepackage{tikz}   
\def\Put(#1,#2)#3{\leavevmode\makebox(0,0){\put(#1,#2){#3}}}
\newcommand*\mystrut[1]{\vrule width0pt height0pt depth#1\relax}
\newcommand{\eqdef}{\mathbin{\stackrel{\rm def}{=}}}


\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\bv}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\mv}{mv}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\step}{step}
\DeclareMathOperator{\gap}{gap}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\orth}{orth}
\newcommand{\norm}[1]{\|#1\|}
\captionsetup[subfigure]{labelformat=empty}
\captionsetup[figure]{labelformat=empty}
\DeclareMathOperator*{\lmin}{\lambda_{min}}
\DeclareMathOperator*{\lmax}{\lambda_{max}}

\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newcommand{\specialcellleft}[2][c]{%
\begin{tabular}[#1]{@{}l@{}}#2\end{tabular}
}

\newtheorem{claim}[theorem]{Claim}
%\newtheorem{corollary}[theorem]{Corollary}

\usepackage{tabstackengine}
\stackMath


%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{CS-GY 9223 I: Lecture 7 \\ Preconditioning, acceleration, coordinate decent, etc.}
\author{NYU Tandon School of Engineering, Prof. Christopher Musco}
\date{}

\begin{document}

\begin{frame}
	\titlepage 
\end{frame}

\metroset{titleformat=smallcaps}

\begin{frame}[t]
	\frametitle{logistics}
	\begin{itemize}
		\item Self-proctored, 2-hour midterm to be taken anytime next week.
		\item \emph{No Collaboration} allowed at all. Or outside resources. Just use your own notes and material from the class. 
		\item Sample problems are available on course website. We can review during office hours tomorrow or next week. 
		\item You should have received an invite to Gradescope. Hopefully tonight/tomorrow I can upload a "practice test" to make sure their system works. 
	\end{itemize}
\end{frame}

\begin{frame}[t]
	\frametitle{gradient descent}
	\textbf{Conditions:}
	\begin{itemize}
		\item \textbf{Convexity:} $f$ is a convex function, $\mathcal{S}$ is a convex set. 
		\item \textbf{Bounded initial distant:} 
		\begin{align*}
			\|\bv{x}^{(0)} - \bv{x}^*\|_2 \leq \alert{R}
		\end{align*}
		\item \textbf{Bounded gradients (Lipschitz function)}: 
		\begin{align*}
			\|\nabla f(\bv{x})\|_2 \leq \alert{G} \text{ for all } \bv{x}\in \mathcal{S}.
		\end{align*}
	\end{itemize}
	
	\begin{theorem}[]GD Convergence Bound]
	(Projected) Gradient Descent returns $\hat{\bv{x}}$ with $f(\hat{\bv{x}}) \leq \min_{\bv{x}\in \mathcal{S}}f(\bv{x})+\epsilon$ after
	\begin{align*}
		T = \frac{R^2 G^2}{\epsilon^2} \text{ iterations.}
	\end{align*}
	\end{theorem}
\end{frame}

\begin{frame}[t]
	\frametitle{online gradient descent}
	$\bv{x}^{*} = \min_\bv{x}\sum_{i=1}^T f_i(\bv{x}^*)$ (the offline optimum)
	
	\textbf{Conditions:}
	\begin{itemize}
		\item $f_1, \ldots, f_T$ are all convex.
		\item Each is $G$-Lipschitz: for all $\bv{x}$, $i$, $\|\nabla f_i(\bv{x})\|_2 \leq \alert{G}$.
		\item Starting radius: $\|\bv{x}^{*} - \bv{x}^{(1)}\|_2 \leq \alert{R}$.
	\end{itemize}
	
	\begin{theorem}[OGD Regret Bound]
	After $T$ steps, $\left[\sum_{i=1}^T f_i(\bv{x}^{(i)})\right] - \left[\sum_{i=1}^T f_i(\bv{x}^*)\right] \leq RG\sqrt{T}$. I.e. the average regret $\frac{1}{T}\left[\sum_{i=1}^T f_i(\bv{x}^{(i)})\right]$ is $\leq \epsilon$ after:	
	\begin{align*}
		T = \frac{R^2 G^2}{\epsilon^2} \text{ iterations.}
	\end{align*}
	\end{theorem}
\end{frame}

\begin{frame}[t]
	\frametitle{stochastic gradient descent}
	\textbf{Conditions:}
	\begin{itemize}
		\item {Finite sum structure:} $f(\bv{x}) = \sum_{i=1}^n f_i(\bv{x})$, with $f_1, \ldots, f_n$ all convex.
		\item {Lipschitz functions}: for all $\bv{x}$, $j$, $\|\nabla f_j(\bv{x})\|_2 \leq \alert{\frac{G'}{n}}$.
		\item Starting radius: $\|\bv{x}^{*} - \bv{x}^{(1)}\|_2 \leq \alert{R}$.
	\end{itemize}
	
	\begin{theorem}[SGD Regret Bound]
		Stochastic Gradient Descent returns $\hat{\bv{x}}$ with $\E[f(\hat{\bv{x}})] \leq \min_{\bv{x}\in \mathcal{S}}f(\bv{x})+\epsilon$ after
	\begin{align*}
		T = \frac{R^2 G'^2}{\epsilon^2} \text{ iterations.}
	\end{align*}	
	We always have that $G' > G$, but iterations are typically cheaper by a factor of $n$. 
\end{theorem}
\end{frame}

\begin{frame}
	\frametitle{beyond the basic bounds}
	Can our convergence bounds be tightened for certain functions? Can they guide us towards faster algorithms?
	
	\textbf{Goals:}
	\begin{itemize}
		\item Improve $\epsilon$ dependence below $1/\epsilon^2$.
		\begin{itemize}
			\item Ideally $1/\epsilon$ or $\log(1/\epsilon)$. 
		\end{itemize}
		\item Reduce or eliminate dependence on $G$ and $R$.
		\item Further take advantage of structure in the data (e.g. repetition in features in addition to data points).  
	\end{itemize}
\end{frame}

\begin{frame}[t]
	\frametitle{smoothness}
	\begin{definition}[$\beta$-smoothness]
		A function $f$ is $\alert{\beta}$ smooth if, for all $\bv{x}$, $\bv{y}$
		\begin{align*}
			\|\nabla f(\bv{x}) - \nabla f(\bv{y})\|_2 \leq \alert{\beta} \|\bv{x} - \bv{y}\|_2
		\end{align*}
	\end{definition}
\vspace{-.5em}
	After some calculus (see Lem. 3.4 in \color{blue}\textbf{\href{https://arxiv.org/pdf/1405.4980.pdf}{Bubeck's book}}\color{black}), this implies:
	\vspace{-1.5em}
	\begin{align*}
		\left[f(\bv{y}) - f(\bv{x})\right] - \nabla f(\bv{x})^T(\bv{y} - \bv{x})  \leq \frac{\beta}{2}\|\bv{x} - \bv{y}\|_2^2
	\end{align*}


\vspace{6em}
For a scalar valued function $f$, equivalent to $f''(x) \leq \beta$. 
\end{frame}

\begin{frame}[t]
	\frametitle{smoothness}
	Recall from definition of convexity that:
	\begin{align*}
		f(\bv{y}) - f(\bv{x}) \geq \nabla f(\bv{x})^T(\bv{y} - \bv{x})
	\end{align*}
	\begin{center}
		\alert{So now we have an upper and lower bound.}
	\end{center}
	\begin{align*}
		0 \leq \left[f(\bv{y}) - f(\bv{x})\right] - \nabla f(\bv{x})^T(\bv{y} - \bv{x}) \leq \frac{\beta}{2}\|\bv{x} - \bv{y}\|_2^2
	\end{align*}
\end{frame}

\begin{frame}[t]
	\frametitle{guaranteed progress}
	Previously learning rate/step size $\eta$ depended on $G$. Now choose it based on $\beta$:
	\begin{align*}
		\bv{x}^{(t+1)} \leftarrow \bv{x}^{(t)} - \frac{1}{\beta}\nabla f(\bv{x}^{(t)})
	\end{align*}
	
	\textbf{Progress per step of gradient descent:}
	\begin{align*}
		\left[f(\bv{x}^{(t+1)}) - f(\bv{x}^{(t)})\right] - \nabla f(\bv{x}^{(t)})^T(\bv{x}^{(t+1)} - \bv{x}^{(t)})  \leq \frac{\beta}{2}\|\bv{x}^{(t)} - \bv{x}^{(t+1)}\|_2^2\\
	\end{align*}
	\begin{align*}
		\left[f(\bv{x}^{(t+1)}) - f(\bv{x}^{(t)})\right] + 
		\frac{1}{\beta}\|\nabla f(\bv{x}^{(t)})\|_2^2 \leq \frac{\beta}{2}\|\frac{1}{\beta}\nabla f(\bv{x}^{(t)})\|_2^2 \\
	\end{align*}
	\begin{align*}
		f(\bv{x}^{(t)}) - f(\bv{x}^{(t+1)}) \geq \alert{\frac{1}{2\beta}\|\nabla f(\bv{x}^{(t)})\|_2^2}
	\end{align*}
	
\end{frame}

\begin{frame}[t]
	\frametitle{convergence guarantee}
	\begin{theorem}[GD convergence for $\beta$-smooth functions.]
		Let $f$ be a \alert{$\beta$} smooth convex function and assume we have $\|\bv{x}^{*} - \bv{x}^{(1)}\|_2 \leq \alert{R}$. If we run GD for $T$ steps with $\eta = \frac{1}{\beta}$ we have:
		\begin{align*}
			f(\bv{x}^{(T)}) - f(\bv{x}^*) \leq \frac{2\beta R^2}{T-1} 
		\end{align*} 
	\end{theorem}
	\textbf{Corollary}: If \alert{$T = O\left(\frac{\beta R^2}{\epsilon}\right)$} we have $f(\bv{x}^{(T)}) - f(\bv{x}^*) \leq \epsilon$.
\end{frame}

\begin{frame}[t]
	\frametitle{strong convexity}
	\begin{definition}[$\alpha$-strongly convex]
		A convex function $f$ is $\alert{\alpha}$-strongly convex if, for all $\bv{x}$, $\bv{y}$
		\begin{align*}
				\left[f(\bv{y}) - f(\bv{x})\right] - \nabla f(\bv{x})^T(\bv{y} - \bv{x}) \geq \frac{\alpha}{2}\|\bv{x} - \bv{y}\|_2^2 
		\end{align*}
	\end{definition}
	$\alpha$ is a parameter that will depend on our function.
	
	\vspace{7em}
	For a twice-differentiable scalar valued function $f$, equivalent to $f''(x) \geq \alpha$. 
\end{frame}

\begin{frame}[t]
	\frametitle{gd for strongly convex function}
	\textbf{Gradient descent for strongly convex functions:}
	\begin{itemize}
		\item Choose number of steps $T$.
		\item For $i = 1,\ldots, T$:
		\begin{itemize}
			\item $\eta = \frac{2}{\alpha\cdot(i+1)}$
			\item $\bv{x}^{(i+1)} = \bv{x}^{(i)} - \eta \nabla f(\bv{x}^{(i)})$
		\end{itemize}
		\item Return $\hat{\bv{x}} = \argmin_{\bv{x}^{(i)}} f(\bv{x}^{(i)})$. 
		%		\item Alternatively, return $\hat{\bv{x}} = \sum_{i=1}^T \frac{2i}{T(T+1)} \bv{x}^{(i)}$.
	\end{itemize}
\end{frame}

\begin{frame}[t]
	\frametitle{convergence guarantee}
	\begin{theorem}[GD convergence for $\alpha$-strongly convex functions.]
		Let $f$ be an \alert{$\alpha$}-strongly convex function and assume we have that, for all $\bv{x}$, $\|\nabla f(\bv{x})\|_2 \leq \alert{G}$. If we run GD for $T$ steps (with adaptive step sizes) we have:
		\begin{align*}
			f(\hat{\bv{x}}) - f(\bv{x}^*) \leq \frac{2G^2}{\alpha(T-1)} 
		\end{align*} 
	\end{theorem}
	\textbf{Corollary}: If \alert{$T = O\left(\frac{G^2}{\alpha \epsilon}\right)$} we have $f(\hat{\bv{x}}) - f(\bv{x}^*) \leq \epsilon$
\end{frame}

\begin{frame}[t]
	\frametitle{convergence guarantee}
		\begin{center}
		What if $f$ is both $\beta$-smooth and $\alpha$-strongly convex?
	\end{center}
	\begin{align*}
		\alert{\frac{\alpha}{2}}\|\bv{x} - \bv{y}\|_2^2 \leq \nabla f(\bv{x})^T(\bv{x} - \bv{y}) - \left[f(\bv{x}) - f(\bv{y})\right] \leq \alert{\frac{\beta}{2}}\|\bv{x} - \bv{y}\|_2^2.
	\end{align*}

\end{frame}

\begin{frame}[t]
	\frametitle{convergence guarantee}
	\begin{align*}
		\alert{\frac{\alpha}{2}}\|\bv{x} - \bv{y}\|_2^2 \leq \nabla f(\bv{x})^T(\bv{x} - \bv{y}) - \left[f(\bv{x}) - f(\bv{y})\right] \leq \alert{\frac{\beta}{2}}\|\bv{x} - \bv{y}\|_2^2.
	\end{align*}
	
	\begin{theorem}[GD for $\beta$-smooth, $\alpha$-strongly convex.]
		Let $f$ be a $\beta$-smooth and $\alpha$-strongly convex function. If we run GD for $T$ steps (with step size $\eta = \frac{1}{\beta}$) we have:
		\begin{align*}
			\|\bv{x}^{(T)} - \bv{x}^*\|_2^2 \leq e^{-(T-1)\frac{\alpha}{\beta}} \|\bv{x}^{(1)} - \bv{x}^*\|_2^2
		\end{align*} 
	\end{theorem}	
	\begin{center}
		\alert{$\kappa = \frac{\beta}{\alpha}$} is called the ``condition number'' of $f$. 
		
		\textbf{Is it better if $\kappa$ is large or small?}
	\end{center}
\end{frame}

\begin{frame}[t]
	\frametitle{smooth and strongly convex}
	\textbf{Converting to more familiar form:}
	Using that fact the $\nabla f(\bv{x}^*) = \bv{0}$ along with
	\begin{align*}
		{\frac{\alpha}{2}}\|\bv{x} - \bv{y}\|_2^2 \leq \nabla f(\bv{x})^T(\bv{x} - \bv{y}) - \left[f(\bv{x}) - f(\bv{y})\right] \leq {\frac{\beta}{2}}\|\bv{x} - \bv{y}\|_2^2, 
	\end{align*}
	we have:
	\begin{align*}
		\|\bv{x}^{(1)} - \bv{x}^*\|_2^2 &\leq \frac{2}{\alpha} \left[f(\bv{x}^{(1)}) - f(\bv{x}^*)\right]\\
		\|\bv{x}^{(T)} - \bv{x}^*\|_2^2 &\geq \frac{2}{\beta} \left[f(\bv{x}^{(T)}) - f(\bv{x}^*)\right]
	\end{align*}	
\end{frame}

\begin{frame}[t]
	\frametitle{convergence guarantee}
	\begin{corollary}[GD for $\beta$-smooth, $\alpha$-strongly convex.]
		Let $f$ be a $\beta$-smooth and $\alpha$-strongly convex function. If we run GD for $T$ steps (with step size $\eta = \frac{1}{\beta}$) we have:
		\begin{align*}
			f(\bv{x}^{(T)}) - f(\bv{x}^*)  \leq \frac{\beta}{\alpha} e^{-(T-1)\frac{\alpha}{\beta}} \cdot  \left[f(\bv{x}^{(1)}) - f(\bv{x}^*)\right]
		\end{align*} 
	\end{corollary}	
	
	\textbf{Corollary}: 
	If \alert{$T = O\left(\frac{\beta}{\alpha}\log(\beta/\alpha \epsilon)\right) =  O(\kappa\log(\kappa/\epsilon))$ } we have:
	\begin{align*}
		f({\bv{x}}^{(T)}) - f(\bv{x}^*) \leq \epsilon \left[f(\bv{x}^{(1)}) - f(\bv{x}^*)\right]
	\end{align*}

	\textbf{Alternative Corollary}: 
	If \alert{$T = O\left(\frac{\beta}{\alpha}\log(R\beta/\epsilon)\right)$} we have:
	\begin{align*}
		f({\bv{x}}^{(T)}) - f(\bv{x}^*) \leq \epsilon
	\end{align*}
	
\end{frame}

\begin{frame}[t]
	\frametitle{the linear algebra of conditioning}
	Let $f$ be a twice differentiable function from $\R^d \rightarrow \R$. Let the \textbf{\alert{Hessian}} $\bv{H} = \nabla^2 f(\bv{x})$ contain all of its second derivatives at a point $\bv{x}$. So $\bv{H}\in \R^{d\times d}$.  We have:
	\begin{align*}
		\bv{H}_{i,j} = \left[\nabla^2 f(\bv{x})\right]_{i,j} = \frac{\partial^2 f}{\partial x_i x_j}. 
	\end{align*}
	
	For vector $\bv{x}, \bv{y}$:
	\begin{align*}
		\nabla f(\bv{x}) - \nabla f(\bv{y}) \approx \left[\nabla^2 f(\bv{x})\right](\bv{x}-\bv{y}).
	\end{align*}
\end{frame}

\begin{frame}[t]
	\frametitle{the linear algebra of conditioning}
	Let $f$ be a twice differentiable function from $\R^d \rightarrow \R$. Let the \textbf{\alert{Hessian}} $\bv{H} = \nabla^2 f(\bv{x})$ contain all of its second derivatives at a point $\bv{x}$. So $\bv{H}\in \R^{d\times d}$.  We have:
	\begin{align*}
		\bv{H}_{i,j} = \left[\nabla^2 f(\bv{x})\right]_{i,j} = \frac{\partial^2 f}{\partial x_i x_j}. 
	\end{align*}
	
	\textbf{Example:}
	Let $f(\bv{x}) = \|\bv{A}\bv{x} - \bv{b}\|_2^2$. Recall that $\nabla f(\bv{x}) = 2\bv{A}^T(\bv{A}\bv{x}-\bv{b}).$
	
	\includegraphics[width=.4\textwidth]{regression.png}
\end{frame}

\begin{frame}[t]
	\frametitle{hessian matrices and positive semidefiniteness}
	\textbf{Claim:} If $f$ is twice differentiable, then it is convex if and only if the matrix $\bv{H} = \nabla^2 f(\bv{x})$ is \emph{positive semidefinite} for all $\bv{x}$. 
	
	\begin{definition}[Positive Semidefinite (PSD)]
		A square, symmetric matrix $\bv{H}\in \R^{d\times d}$ is \emph{positive semidefinite} (PSD) for any vector $\bv{y}\in \R^d$, $\bv{y}^T\bv{H}\bv{y} \geq 0$. 
	\end{definition}
	
	This is a natural notion of ``positivity'' for symmetric matrices. To denote that $\bv{H}$ is PSD we will typically use ``Loewner order'' notation (\texttt{\textbackslash succeq} in LaTex): $$\bv{H}\succeq 0.$$
	
	We write $\bv{B}\succeq \bv{A}$ or equivalently  $\bv{A}\succeq \bv{B}$ to denote that $(\bv{B} - \bv{A})$ is positive semidefinite. This gives a \emph{partial ordering} on matrices.
\end{frame}

\begin{frame}[t]
	\frametitle{hessian matrices and positive semidefiniteness}
	\textbf{Claim:} If $f$ is twice differentiable, then it is convex if and only if the matrix $\bv{H} = \nabla^2 f(\bv{x})$ is \emph{positive semidefinite} for all $\bv{x}$. 
	
	\begin{definition}[Positive Semidefinite (PSD)]
		A square, symmetric matrix $\bv{H}\in \R^{d\times d}$ is \emph{positive semidefinite} (PSD) for any vector $\bv{y}\in \R^d$, $\bv{y}^T\bv{H}\bv{y} \geq 0$. 
	\end{definition}

	For the least squares regression loss function: $f(\bv{x}) = \|\bv{A}\bv{x} - \bv{b}\|_2^2$, $\bv{H} = \nabla^2 f(\bv{x})= 2\bv{A}^T\bv{A}$ for all $\bv{x}$. Is $\bv{H}$ PSD?
\end{frame}

\begin{frame}[t]
	\frametitle{the linear algebra of conditioning}
	If $f$ is $\beta$-smooth and $\alpha$-strongly convex then at any point $\bv{x}$, $\bv{H} = \nabla^2 f(\bv{x})$ satisfies: 
	\begin{align*}
		\alpha\bv{I}_{d\times d} \preceq \bv{H} \preceq \beta\bv{I}_{d\times d},
	\end{align*}
where $\bv{I}_{d\times d}$ is a $d\times d$ identity matrix. 
	
	This is the natural matrix generalization of the statement for scalar valued functions:
	\begin{align*}
	\alpha \leq f''(x)\leq \beta.
	\end{align*}

\end{frame}

\begin{frame}[t]
	\frametitle{smooth and strongly convex hessian}
	\begin{align*}
		\alpha\bv{I}_{d\times d} \preceq \bv{H} \preceq \beta\bv{I}_{d\times d}.
	\end{align*}
Equivalently for any $\bv{z}$,
\begin{align*}
	\alpha\|\bv{z}\|_2^2 \leq \bv{z}^T\bv{H}\bv{z} \leq \beta\|\bv{z}\|_2^2.
\end{align*} 
	\textbf{Exercise:} Show that for $f(\bv{x}) = \|\bv{A}\bv{x} - \bv{b}\|_2^2$,
	\begin{align*}
		\left[f(\bv{x}) - f(\bv{y})\right] - \nabla f(\bv{x})^T(\bv{y} - \bv{x})  &= (\bv{x}-\bv{y})^T\left[2\bv{A}^T\bv{A}\right](\bv{x}-\bv{y}).
	\end{align*}

This would imply:

	\begin{align*}
	\frac{\alpha}{2} \|\bv{x}-\bv{y}\|_2^2 \leq \left[f(\bv{x}) - f(\bv{y})\right] - \nabla f(\bv{x})^T(\bv{y} - \bv{x})  \leq \frac{\beta}{2}\|\bv{x}-\bv{y}\|_2^2
	\end{align*}
\end{frame}

\begin{frame}[t]
	\frametitle{simple example}
	Let $f(\bv{x}) = \|\bv{D}\bv{x} - \bv{b}\|_2^2$ where $\bv{D}$ is a diagaonl matrix. For now imagine we're in two dimensions: $\bv{x} = \begin{bmatrix}
		x_1\\
		x_2
	\end{bmatrix}$, $\bv{D} = \begin{bmatrix}
		d_1 & 0 \\
		0 & d_2
	\end{bmatrix}
	$.
	\begin{center}
		\textbf{What are $\alpha,\beta$ for this problem?}
	\end{center}
	\begin{align*}
		\alpha\|\bv{z}\|_2^2 \leq \bv{z}^T\bv{H}\bv{z} \leq \beta\|\bv{z}\|_2^2
	\end{align*}
\end{frame}

\begin{frame}[t]
	\frametitle{geometric view}
	\begin{center}
		\includegraphics[width=.5\textwidth]{perfect_conditioning.png}
		
		Level sets of $\|\bv{D}\bv{x} - \bv{b}\|_2^2$ when $d_1^2 = 1, d_2^2 = 1$. 
	\end{center}
\end{frame}

\begin{frame}[t]
	\frametitle{geometric view}
	\begin{center}
		\includegraphics[width=\textwidth]{poor_conditioning.png}
		
		Level sets of $\|\bv{D}\bv{x} - \bv{b}\|_2^2$ when $d_1^2 = \frac{1}{3}, d_2^2 = 2$. 
	\end{center}
\end{frame}

\begin{frame}[t]
	\frametitle{eigendecomposition view}
	Any symmetric matrix $\bv{H}$ has an \emph{orthogonal}, real valued eigendecomposition. 
	\begin{center}
		\includegraphics[width=.9\textwidth]{eigendecomp.png}
	\end{center}
Here $\bv{V}$ is square and orthogonal, so $\bv{V}^T\bv{V} = \bv{V}\bv{V}^T = \bv{I}$. And for each $\bv{v}_i$, we have:
\begin{align*}
	\bv{H}\bv{v}_i = \lambda_i \bv{v}_i. 
\end{align*}
That's what makes $\bv{v}_1, \ldots, \bv{v}_d$ eigenvectors.	
\end{frame}

\begin{frame}[t]
	\frametitle{eigendecomposition view}
	Recall $\bv{V}\bv{V}^T = \bv{V}^T\bv{V} = \bv{I}$.
	\begin{center}
		\includegraphics[width=.9\textwidth]{eigendecomp.png}
	\end{center}
	\textbf{Claim:}	$\bv{H} \Leftrightarrow \lambda_1, ..., \lambda_d \geq 0$. 
\end{frame}

\begin{frame}[t]
	\frametitle{eigendecomposition view}
	Recall $\bv{V}\bv{V}^T = \bv{V}^T\bv{V} = \bv{I}$.
	\begin{center}
		\includegraphics[width=.9\textwidth]{eigendecomp.png}
	\end{center}
	\textbf{Claim:}	$\alpha\bv{I} \preceq \bv{H} \preceq \beta \bv{I} \Leftrightarrow \alpha \leq \lambda_1, ..., \lambda_d \leq \beta$. 
\end{frame}

\begin{frame}[t]
	\frametitle{eigendecomposition view}
	Recall $\bv{V}\bv{V}^T = \bv{V}^T\bv{V} = \bv{I}$.
	\begin{center}
		\includegraphics[width=.9\textwidth]{eigendecomp.png}
	\end{center}
In other words, if we let $\lmax(\bv{H})$ and $\lmin(\bv{H})$ be the smallest and largest eigenvalues of $\bv{H}$, then for all $\bv{z}$ we have: 
\begin{align*}
	\bv{z}^T\bv{H}\bv{z} &\leq \lmax(\bv{H})\cdot \|\bv{z}\|^2 \\
	\bv{z}^T\bv{H}\bv{z} &\geq \lmin(\bv{H})\cdot \|\bv{z}\|^2 
\end{align*}

\end{frame}


\begin{frame}[t]
	\frametitle{eigendecomposition view}
	If $f(\bv{x})$ is $\beta$-smooth and $\alpha$-strongly convex, then for any $\bv{x}$ we have the the maximum eigenvalue of $\bv{H} = \nabla^2f(\bv{x}) = \beta$ and the minimum eigenvalue of $\bv{H} = \nabla^2f(\bv{x}) = \alpha$. 
	
	\begin{align*}
		\lmax(\bv{H}) &= \beta\\
		\lmin(\bv{H}) &= \alpha
	\end{align*}
\end{frame}

\begin{frame}[t]
	\frametitle{polynomial view point}
	\begin{theorem}[GD for $\beta$-smooth, $\alpha$-strongly convex.]
		Let $f$ be a $\beta$-smooth and $\alpha$-strongly convex function. If we run GD for $T$ steps (with step size $\eta = \frac{1}{2\beta}$) we have:
		\begin{align*}
		\|\bv{x}^{(T)} - \bv{x}^*\|_2 \leq e^{-T/\kappa} \|\bv{x}^{(1)} - \bv{x}^*\|_2
		\end{align*} 
	\end{theorem}
	
	\begin{center}
		\alert{\textbf{Goal: Prove for $f(\bv{x}) = \|\bv{A}\bv{x} - \bv{b}\|_2^2$.}}
	\end{center}	
\end{frame}

\begin{frame}[t]
	\frametitle{alternative view of gradient descent}
	\textbf{Richardson Iteration view:}
	\begin{align*}
	(\bv{x}^{(T+1)} - \bv{x}^*) =  \left(\bv{I} - \frac{1}{\lmax}\bv{A}^T\bv{A}\right)(\bv{x}^{(t)} - \bv{x}^*) 
	\end{align*}


\vspace{10em}
	What is the maximum eigenvalue of the symmetric matrix $\left(\bv{I} - \frac{1}{\lmax}\bv{A}^T\bv{A}\right)$ in terms of the eigenvalues $\lmax = \lambda_1 \geq \ldots \geq \lambda_d = \lmin$ of $\bv{A}^T\bv{A}$?
\end{frame}

\begin{frame}[t]
	\frametitle{unrolled gradient descent}
	\begin{align*}
	(\bv{x}^{(T+1)} - \bv{x}^*) =  \left(\bv{I} - \frac{1}{\lmax}\bv{A}^T\bv{A}\right)^T(\bv{x}^{(1)} - \bv{x}^*) 
	\end{align*}
	
\vspace{3em}
What is the maximum eigenvalue of the symmetric matrix $\left(\bv{I} - \frac{1}{\lmax}\bv{A}^T\bv{A}\right)^T$?

\vspace{5em}
So we have $\|\bv{x}^{(T)} - \bv{x}^*\|_2 \leq $
\end{frame}

\begin{frame}
	\frametitle{improving gradient descent}
	We now have a \emph{really good} understanding of gradient descent. 
	
\textbf{Number of iterations for $\epsilon$ error:}
\begin{center}
	\begin{tabular}{c|cc}
		& $G$-Lipschitz & $\beta$-smooth   \\ \hline
		$R$ bounded start & $O\left(\frac{G^2R^2}{\epsilon^2}\right)$ & $O\left(\frac{\beta R^2}{\epsilon}\right)$ \\
		$\alpha$-strong convex & $O\left(\frac{G^2}{\alpha\epsilon}\right)$ & $O\left(\frac{\beta}{\alpha}\log(1/\epsilon)\right)$
	\end{tabular}
\end{center}

\vspace{1em}
\alert{How do we use this understanding to design \emph{faster algorithms?}}
\end{frame}

\begin{frame}[standout]
	\begin{center}
		\large acceleration
	\end{center}
\end{frame}


\begin{frame}
	\frametitle{accelerated gradient descent}
	\textbf{Nesterov's accelerated gradient descent}:
	\begin{itemize}
		\item $\bv{x}^{(1)} = \bv{y}^{(1)} = \bv{z}^{(1)}$  
		\item For $t = 1,\ldots, T$
		\begin{itemize}
			\item $\bv{y}^{(t+1)} = \bv{x}^{(t)} - \frac{1}{\beta}\nabla f(\bv{x}^{(t)})$
			\item $\bv{x}^{(t+1)} = \left(1 + \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right) \bv{y}^{(t+1)} + \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\left(\bv{y}^{(t+1)} - \bv{y}^{(t)}\right)$
		\end{itemize}
	\end{itemize}
	\begin{theorem}[AGD for $\beta$-smooth, $\alpha$-strongly convex.]
		Let $f$ be a $\beta$-smooth and $\alpha$-strongly convex function. If we run AGD for $T$ steps we have:
		\begin{align*}
			f(\bv{x}^{(t)}) - f(\bv{x}^*) \leq \kappa e^{-(t-1)\sqrt{\kappa}} \left[f(\bv{x}^{(1)}) - f(\bv{x}^*) \right]
		\end{align*} 
	\end{theorem}	
	\textbf{Corollary:} If \alert{$T = O\left(\sqrt{\kappa}\log(\kappa/\epsilon)\right)$ achieve error $\epsilon$.} 
	
\end{frame}
%
%\begin{frame}[t]
%	\frametitle{linear regression runtime}
%	\textbf{Total runtime for solving linear regression via GD}:
%	\begin{center}
%		(time per iteration) x (number of iterations)
%		
%		\vspace{5em}
%		
%		\large
%		$\alert{O(nd\cdot\kappa\log(1/\epsilon))}$
%		
%		for $\bv{A}\in \R^{n\times d}$, $\bv{x} \in \R^d$, $\bv{b}\in \R^n$.
%	\end{center}	
%\end{frame}
%
%\begin{frame}[t]
%	\frametitle{acceleration}
%	\begin{theorem}[Accelerated Iterative Regression]
%		Let $\bv{x}^* = \min_{\bv{x}}\|\bv{A}\bv{x} - \bv{b}\|_2^2$. There is an algorithm which finds $\tilde{\bv{x}} $ with $\|\tilde{\bv{x}} - \bv{x}^*\|_2 \leq \epsilon\|\bv{x}^*\|_2$ in time:
%			\begin{align*}
%				O(nd\cdot\sqrt{\kappa\log(1/\epsilon)})
%			\end{align*} 
%	\end{theorem}
%\end{frame}
%
%\begin{frame}[t]
%	\frametitle{the polynomial view}
%	\textbf{Claim:} For any $\eta$, polynomial $p(z) = c_1 z + c_2 z^2 + \ldots + c_q z^q$ with $p(1) = \sum_{j=1}^q c_q = 1$, there is an algorithm running in $O(ndq)$ time which outputs $\tilde{\bv{x}}$ satisfying:
%	\begin{align*}
%	\tilde{\bv{x}} - \bv{x}^* =  p(\bv{I} - \frac{1}{\eta}\bv{A}^T\bv{A}) \bv{x}^*
%	\end{align*}
%	
%	\begin{center}
%		\alert{For standard gradient descent, $p(z) = z^q$.}
%	\end{center}
%\end{frame}
%
%\begin{frame}[t]
%	\frametitle{the polynomial view}
%	\textbf{Claim:} For any $\eta$, polynomial $p(z) = c_1 z + c_2 z^2 + \ldots + c_q z^q$ with $p(1) = \sum_{j=1}^q c_q = 1$, there is an algorithm running in $O(ndq)$ time which outputs $\tilde{\bv{x}}$ satisfying:
%	\begin{align*}
%	\tilde{\bv{x}} - \bv{x}^* =  c_1\cdot (\bv{I} - \eta\bv{A}^T\bv{A})\bv{x}^* + c_2 \cdot (\bv{I} - \eta\bv{A}^T\bv{A})^2  \bv{x}^* + \ldots + c_q \cdot (\bv{I} - \eta\bv{A}^T\bv{A})^q  \bv{x}^* 
%	\end{align*}
%	
%	\textbf{Claim:} $c_j  \cdot \bv{I} - \eta\bv{A}^T\bv{A})^j\bv{x}^* = c_j\cdot\bv{x}^* + p_j'(\bv{I} - \eta\bv{A}^T\bv{A})\bv{A}^T\bv{A}\bv{x}^*$ where $p_j$ is a polynomial with degree $j - 1$.
%\end{frame}
%
%\begin{frame}[t]
%	\frametitle{the polynomial view}
%	\textbf{Claim:} For any $\eta$, polynomial $p(z) = c_1 z + c_2 z^2 + \ldots + c_q z^q$ with $p(1) = \sum_{j=1}^q c_q = 1$, there is an algorithm running in $O(ndq)$ time which outputs $\tilde{\bv{x}}$ satisfying:
%	\begin{align*}
%	\bv{x}^* - \tilde{\bv{x}} &=  (c_1 + c_2 + \ldots + c_q)\cdot\bv{x}^* +  p'(\bv{I} - \eta\bv{A}^T\bv{A})\bv{A}^T\bv{A}\bv{x}^*
%	\end{align*}
%	\begin{align*}
%		\tilde{\bv{x}}  &= p'(\bv{I} - \eta\bv{A}^T\bv{A})\bv{A}^T\bv{b} \text{ where $p'$ is a polynmial with degree $q-1$}.
%	\end{align*}
%\end{frame}
%
%\begin{frame}[t]
%	\frametitle{the polynomial view}
%	\begin{align*}
%	\tilde{\bv{x}} - \bv{x}^* =  p(\bv{I} - \eta\bv{A}^T\bv{A})\bv{x}^*\\
%	p(\bv{I} - \eta\bv{A}^T\bv{A}) = \bv{U}p(\bv{I} - \eta\bs{\Lambda})\bv{U}^T
%	\end{align*}
%	
%	\begin{align*}
%	\|\tilde{\bv{x}} - \bv{x}^*\|  &= \|\bv{U}p(\bv{I} - \eta\bs{\Lambda})\bv{U}^T\bv{x}^*\|_2 \\
%	& = \|p(\bv{I} - \eta\bs{\Lambda})\bv{U}^T\bv{x}^*\|_2
%	\end{align*}
%	
%	As long as $\max\left[ p(\bv{I} - \eta\bs{\Lambda})\right] \leq \epsilon$, 
%	\begin{align*}
%	\|\tilde{\bv{x}} - \bv{x}^*\|_2  \leq \epsilon \|\bv{x}^*\|_2 
%	\end{align*}
%\end{frame}
%
%\begin{frame}[t]
%	\frametitle{constructing a jump polynomial}
%	\textbf{Goal:} Find polynomial $p$ such that $p(1) = 1$ and $p(z) \leq \epsilon$ for $z\in [0,1 - \frac{1}{\kappa}]$.
%	\begin{center}
%		\includegraphics[width=.6\textwidth]{basic_jump.png}
%		
%		Gradient descent uses $p(z) = z^{O(\kappa\log(1/\epsilon))}$.
%	\end{center}
%\end{frame}
%
%\begin{frame}[t]
%	\frametitle{a better jump polynomial}
%		\textbf{Goal:} Find polynomial $p$ such that $p(1) = 1$ and $p(z) \leq \epsilon$ for $z\in [0,1 - \frac{1}{\kappa}]$.
%	\begin{center}
%		\includegraphics[width=.6\textwidth]{cheby_jump.png}
%		
%		\alert{Can be done with degree $O(\sqrt{\kappa\log(1/\epsilon)})$ polynomial instead!}
%	\end{center}
%\end{frame}
%
%\begin{frame}
%	\frametitle{chebyshev polynomials}
%	\begin{center}
%		\textbf{What are these polynomials?}
%	\end{center}
%	
%		\centering
%		Chebyshev polynomials of the first kind.
%		\begin{columns}
%			\begin{column}{0.5\textwidth}
%				\begin{align*}
%				T_0(x) &= 1\\
%				T_1(x) &= x \\
%				T_2(x) &= 2x^2 - 1\\
%				&\,\,\,\vdots\\
%				T_k(x) &= 2xT_{k-1}(x) - T_{k-2}(x)\\
%				\end{align*}
%			\end{column}
%			\begin{column}{0.5\textwidth}
%				\includegraphics[width=\textwidth]{chebyPolys.png}
%			\end{column}
%		\end{columns}
%		\begin{center}
%			\textbf{``There's only one bullet in the gun. It's called the Chebyshev polynomial.''} -- Prof. Rocco Servedio
%		\end{center}
%\end{frame}
%
%\begin{frame}
%	\frametitle{accelerated gradient descent}
%	\textbf{Nesterov's accelerated gradient descent}:
%	\begin{itemize}
%		\item $\bv{x}^{(1)} = \bv{y}^{(1)} = \bv{z}^{(1)}$  
%		\item For $t = 1,\ldots, T$
%		\begin{itemize}
%			\item $\bv{y}^{(t+1)} = \bv{x}^{(t)} - \frac{1}{\beta}\nabla f(\bv{x}^{(t)})$
%			\item $\bv{x}^{(t+1)} = \left(1 + \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right) \bv{y}^{(t+1)} - \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\bv{y}^{(t)}$
%		\end{itemize}
%	\end{itemize}
%	\begin{theorem}[AGD for $\beta$-smooth, $\alpha$-strongly convex.]
%	Let $f$ be a $\beta$-smooth and $\alpha$-strongly convex function. If we run AGD for $T$ steps we have:
%	\begin{align*}
%	f(\bv{x}^{(t)}) - f(\bv{x}^*) \leq \kappa e^{-(t-1)\sqrt{\kappa}} \left[f(\bv{x}^{(1)}) - f(\bv{x}^*) \right]
%	\end{align*} 
%\end{theorem}	
%\textbf{Corollary:} If \alert{$T = O\left(\sqrt{\kappa}\log(\kappa/\epsilon)\right)$ achieve error $\epsilon$.} 
%	
%\end{frame}

\begin{frame}[t]
	\frametitle{intuition behind acceleration}
	\begin{center}
		\includegraphics[width=\textwidth]{poor_conditioning.png}
		
		Level sets of $\|\bv{A}\bv{x} - \bv{b}\|_2^2$.
	\end{center}

	\textbf{Other terms for similar ideas:}
	\begin{itemize}
		\item Momentum
		\item Heavy-ball methods
	\end{itemize}

	\begin{center}
		\alert{What if we look back beyond \emph{two iterates}?}
	\end{center}
\end{frame}

\begin{frame}[standout]
	\begin{center}
		\large preconditioning
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{preconditioning}
	\textbf{Main idea:}
	Instead of minimizing $f(\bv{x})$, find another function $g(\bv{x})$ with the same minimum but which is better suited for first order optimization (e.g., has a smaller conditioner number).
	
	\vspace{2em}
	\textbf{Claim:} Let $h(\bv{x}): \R^d \rightarrow \R^d$ be an \emph{invertible function}. Let $g(\bv{x}) = f(h(\bv{x}))$. Then
	\begin{align*}
	\alert{\min_{\bv{x}} f(\bv{x})} &\alert{= \min_{\bv{x}} g(\bv{x})} & &\text{and} & \alert{\argmin_{\bv{x}} f(\bv{x})} &\alert{= h\left(\argmin_{\bv{x}} g(\bv{x})\right)}.
	\end{align*}
\end{frame}

\begin{frame}[t]
	\frametitle{preconditioning}
	\textbf{First Goal:} We need $g(\bv{x})$ to still be convex.
	
	\textbf{Claim:} Let $\bv{P}$ be an invertible $d\times d$ matrix and let $g(\bv{x}) = f(\bv{P}\bv{x})$. 
	\begin{center} 
		\alert{$g(\bv{x})$ is always convex.}
	\end{center}
\end{frame}

\begin{frame}[t]
	\frametitle{preconditioning}
	\textbf{Second Goal:} 
	
	$g(\bv{x})$ should have better condition number $\kappa$ than $f(\bv{x})$. 
	
	\textbf{Example:} 
	\begin{itemize}
		\item $f(\bv{x}) = \|\bv{A}\bv{x} - \bv{b}\|_2^2$. $\kappa_f =\frac{\lambda_1(\bv{A}^T\bv{A})}{\lambda_d(\bv{A}^T\bv{A})}$.
		\item $g(\bv{x}) = \|\bv{A}\bv{P}\bv{x} - \bv{b}\|_2^2$. $\kappa_g =\frac{\lambda_1(\bv{P}^T\bv{A}^T\bv{A}\bv{P})}{\lambda_d(\bv{P}^T\bv{A}^T\bv{A}\bv{P})}$.
	\end{itemize}
	
	\textbf{Ideal preconditioner:} Choose $P$ so that  $\bv{P}^T\bv{A}^T\bv{A}\bv{P} = \bv{I}$. For example, could set $P = \sqrt{(\bv{A}^T\bv{A})^{-1}}$.
	
	\begin{center}
		\alert{What's the problem with this choice?}
	\end{center}
\end{frame}

\begin{frame}[t]
	\frametitle{diagonal preconditioner}
	\textbf{Third Goal:} $\bv{P}$ should be easy to compute. 
	
	\begin{center}
		\alert{Many, many problem specific preconditioners are used in practice. There design is usually a heuristic process.} 
	\end{center}
	
	\textbf{Example:} Diagonal preconditioner. 
	\begin{itemize}
		\item Let $\bv{D} = \diag(\bv{A}^T\bv{A})$
		\item Intuitively, we roughly have that $\bv{D} \approx \bv{A}^T\bv{A}$. 
		\item Let $\bv{P} = \sqrt{\bv{D}^{-1}}$
	\end{itemize}
	$\bv{P}$ is often called a \alert{\textbf{Jacobi preconditioner}}. Often works very well in practice!
\end{frame}

\begin{frame}
	\frametitle{diagonal preconditioner}
	\begin{center}
		\includegraphics[width=.7\textwidth]{poorly_cond_a.png}
	\end{center}
\vspace{2em}

  \begin{columns}[t]
	\begin{column}{0.3\textwidth}
		\vspace{-6.4em}
		
		\includegraphics[width=.8\textwidth]{cond1.png}
	\end{column}
	\begin{column}{0.7\textwidth}
		\includegraphics[width=.8\textwidth]{cond2.png}
	\end{column}
\end{columns}
\end{frame}

%\begin{frame}
%	\frametitle{diagonal preconditioner}
%	\begin{center}
%		\alert{Can you think of an example $\bv{A}$ where Jacobi preconditioning doesn't decrease a large $\kappa$?}
%	\end{center}
%	\vspace{3em}
%		\begin{center}
%		\alert{Can Jacobi preconditioning \emph{increase} $\kappa$?}
%	\end{center}
%\end{frame}


\begin{frame}
	\frametitle{adaptive stepsizes}
	\textbf{Another view}: If $g(\bv{x}) = f(\bv{P}\bv{x})$ then $\nabla g(\bv{x}) = \bv{P}^T\nabla f(\bv{P}\bv{x})$.
	
	 $\nabla g(\bv{x})  = \bv{P}\nabla f(\bv{P}\bv{x})$ when $\bv{P}$ is symmetric. 
	
	\vspace{1em}
	\textbf{Gradient descent on $g$:}
	\begin{itemize}
		\item For $t = 1,\ldots, T$,
		\begin{itemize}
			\item $\bv{x}^{(t+1)} = \bv{x}^{(t)} - \eta\bv{P}\left[\nabla f(\bv{P}\bv{x}^{(t)})\right]$
		\end{itemize}
	\end{itemize}

	\vspace{1em}
	\textbf{Gradient descent on $g$:}
	\begin{itemize}
	\item For $t = 1,\ldots, T$,
	\begin{itemize}
		\item $\bv{y}^{(t+1)} = \bv{y}^{(t)} - \eta\bv{P}^2\left[\nabla f(\bv{y}^{(t)})\right]$
	\end{itemize}
	\end{itemize}
	\begin{center}
		\alert{When $\bv{P}$ is diagonal, this is just gradient descent with a \emph{different step size for each parameter}!}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{adaptive stepsizes}
	\textbf{Algorithms based on this idea:}
	\begin{itemize}
		\item AdaGrad
		\item RMSprop
		\item Adam optimizer
	\end{itemize}
\begin{center}
	\includegraphics[width=.5\textwidth]{neuralNetwork.png}
	
	\alert{(Pretty much all of the most widely used optimization methods for training neural networks.)}
\end{center}
\end{frame}

\begin{frame}[standout]
	\begin{center}
		\large coordinate descent
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{stochastic methods}
	\textbf{Main idea:} Trade slower convergence (more iterations) for cheaper iterations. 
	\vspace{2em}
	
	\textbf{Stochastic Gradient Descent:}
	When $f(\bv{x}) = \sum_{i=1}^n f_i(\bv{x})$, approximate $\nabla f(\bv{x})$ with $\nabla f_i(\bv{x})$ for randomly chosen $i$. 
\end{frame}

\begin{frame}
	\frametitle{stochastic methods}
	\textbf{Main idea:} Trade slower convergence (more iterations) for cheaper iterations. 
	\vspace{2em}
	
	\textbf{Stochastic \alert{Coordinate Descent}:}
	Only compute a \emph{single random entry} of $\nabla f(\bv{x})$ on each iteration:
	\begin{align*}
	\nabla f(\bv{x}) &= 
	\begin{bmatrix}
	\frac{\partial f}{\partial x_1}(\bv{x}) \\ \frac{\partial f}{\partial x_2}(\bv{x}) \\ \vdots \\ \frac{\partial f}{\partial x_d}(\bv{x})  
	\end{bmatrix} & 	\nabla_i f(\bv{x}) &= 
	\begin{bmatrix}
	0\\ \frac{\partial f}{\partial x_i}(\bv{x}) \\ \vdots \\ 0
	\end{bmatrix} 
	\end{align*}
	
\textbf{Update:} $\bv{x}^{(t+1)}\leftarrow \bv{x}^{(t)} + \eta \nabla_i f(\bv{x}^{(t)})$.
\end{frame}

\begin{frame}[t]
	\frametitle{coordinate descent}
	When $\bv{x}$ has $d$ parameters, computing $\nabla_i f(\bv{x})$ often costs just a $1/d$ fraction of what it costs to compute $\nabla f(\bv{x})$ 
	
	\vspace{1em}
	\textbf{Example:} $f(\bv{x}) = \|\bv{A}\bv{x} - \bv{b}\|_2^2$ for $\bv{A} \in \R^{n\times d}, \bv{x} \in \R^{d}, \bv{b} \in \R^n$. 
	\begin{itemize}
		\item $\nabla f(\bv{x}) = 2\bv{A}^T\bv{A}\bv{x} - 2\bv{A}^T\bv{b}$.
		\item $\nabla_i f(\bv{x}) = 2\left[\bv{A}^T\bv{A}\bv{x}\right]_i - 2\left[\bv{A}^T\bv{b}\right]$.
	\end{itemize}
\end{frame}

\begin{frame}[t]
	\frametitle{stochastic coordinate descent}	
	\textbf{Stochastic Coordinate Descent:}
	\begin{itemize}
		\item Choose number of steps $T$ and step size $\eta$.
		\item For $i = 1,\ldots, T$:
		\begin{itemize}
			\item Pick random $j_i \in 1, \ldots, d$.
			\item $\bv{x}^{(i+1)} = \bv{x}^{(i)} - \eta \nabla_{j_i} f(\bv{x}^{(i)})$
		\end{itemize}
		\item Return $\hat{\bv{x}} = \frac{1}{T}\sum_{i=1}^T \bv{x}^{(i)}$.
	\end{itemize}
\end{frame}

\begin{frame}[t]
	\frametitle{coordinate descent}
	\begin{theorem}[Stochastic Coordinate Descent convergence]
		Given a $G$-Lipschitz function $f$ with minimizer $\bv{x}^*$ and initial point $\bv{x}^{(1)} $ with $\|\bv{x}^{(1)} - \bv{x}^*\|_2 \leq R$, SCD with step size $\eta = \frac{1}{Rd}$ satisfies the guarantee:
		\begin{align*}
		\E[f(\hat{\bv{x}}) - f({\bv{x}^*})] \leq \frac{2GR}{\sqrt{T/d}}
		\end{align*}
	\end{theorem}
\end{frame}

\begin{frame}[t]
	\frametitle{importance sampling}
	Often it doesn't make sense to sample $i$ uniformly at random:
	\begin{align*}
	\bv{A} &= \begin{bmatrix} 
	0 & 0 & 1 & 0 & 0 & 0 \\
	0 & 0 & 2 & 0 & 0 & 0 \\
	0 & 0 & -1 & 0 & 0 & 0 \\
	0 & 0 & -.5 & 0 & 0 & 0 \\
	0 & 0 & 3 & 0 & 0 & 0 \\
	0 & 0 & -2 & 0 & 0 & 0 
	\end{bmatrix} & 
	\bv{b} &=\begin{bmatrix} 
	 10  \\
	 42 \\
	-11  \\
	 -51 \\
	34\\
	-22 
	\end{bmatrix} 
	\end{align*}
	Select indices $i$ proportional to $\|\bv{a}_i\|_2^2$:
	\begin{align*}
		\Pr[\text{select index $i$ to update}] = \frac{\|\bv{a}_i\|_2^2}{\sum_{j=1}^d \|\bv{a}_j\|_2^2} = \frac{\|\bv{a}_i\|_2^2}{\|\bv{A}\|_2^2} 
	\end{align*}
\end{frame}

\end{document} 








